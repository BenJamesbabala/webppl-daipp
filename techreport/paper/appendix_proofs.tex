\section{Appendix: Gradient Estimator Derivations \& Correctness Proofs}
\label{sec:appendix_proofs}

\newtheorem{lemma}{Lemma}

\subsection{Derivation of Unified Gradient Estimator (Equation~\ref{eq:hybridEstimator})}
\label{sec:appendix:estDerivation}

\begin{align}
\gradparams \elbo
%
&= \gradparams \expect_\reparamDist [ \log p(\xformedVars, \observedVars) - \log \guide(\xformedVars | \observedVars) ]\nonumber\\
%
&= \gradparams \int_\reparamVars \reparamDist(\reparamVars | \observedVars) ( \log p(\xformedVars, \observedVars) - \log \guide(\xformedVars | \observedVars) )\nonumber\\
%
&= \int_\reparamVars \gradparams \reparamDist(\reparamVars | \observedVars) ( \log p(\xformedVars, \observedVars) - \log \guide(\xformedVars | \observedVars) ) + \reparamDist(\reparamVars | \observedVars) \gradparams ( \log p(\xformedVars, \observedVars) - \log \guide(\xformedVars | \observedVars) )\nonumber \\
%
\label{eq:estDerivation_trick}
&= \int_\reparamVars \reparamDist(\reparamVars | \observedVars) \gradparams \log \reparamDist(\reparamVars | \observedVars) ( \log p(\xformedVars, \observedVars) - \log \guide(\xformedVars | \observedVars) ) + \reparamDist(\reparamVars | \observedVars) \gradparams ( \log p(\xformedVars, \observedVars) - \log \guide(\xformedVars | \observedVars) ) \\
%
&= \expect_\reparamDist [ \gradparams \log \reparamDist(\reparamVars | \observedVars) ( \log p(\xformedVars, \observedVars) - \log \guide(\xformedVars | \observedVars) ) + \gradparams( \log p(\xformedVars, \observedVars) - \log \guide(\xformedVars | \observedVars) )]\nonumber\\
%
&= \expect_\reparamDist [ \gradparams \log \reparamDist(\reparamVars | \observedVars) W(\reparamVars, \observedVars) + \gradparams \log p(\xformedVars, \observedVars) - \gradparams \log \guide(\xformedVars | \observedVars) ]\nonumber
\end{align}
Line~\ref{eq:estDerivation_trick} makes use of the identity $\nabla f(x) = f(x) \nabla \log f(x)$.

\subsection{Zero Expectation Identities}
\label{sec:appendix:zeroexp}

In what follows, we will make frequent use of the following:
%%%
\begin{lemma}
If $f(x)$ is a probability distribution, then:

\begin{equation*}
\expect_f[\nabla \log f(x)] = 0
\end{equation*}
\label{lem:zeroexp}
\end{lemma}
%%%
\begin{proof}
\begin{equation*}
\expect_f[\nabla \log f(x)]
%
= \int_x f(x) \nabla \log f(x)
%
= \int_x \nabla f(x)
%
= \nabla \int_x f(x)
%
= \nabla 1
%
= 0
\end{equation*}
\end{proof}

\begin{lemma}
For a factor $\reparamDist(\reparamVar_i | \reparamVars_{<i}, \observedVars)$ of $\reparamDist(\reparamVars | \observedVars)$ and a function $f(\reparamVars_{<i}, \observedVars)$:

\begin{equation*}
\expect_\reparamDist [ \gradparams \log \reparamDist(\reparamVar_i | \reparamVars_{<i}, \observedVars) f(\reparamVars_{<i}, \observedVars) ] = 0
\end{equation*}
\label{lem:zeroexp2}
\end{lemma}
%%%
\begin{proof}
\begin{align*}
\expect_\reparamDist [ \gradparams \log \reparamDist(\reparamVar_i | \reparamVars_{<i}, \observedVars) f(\reparamVars_{<i}, \observedVars) ]
%
&= \int_\reparamVars \reparamDist(\reparamVars | \observedVars) \gradparams \log \reparamDist(\reparamVar_i | \reparamVars_{<i}, \observedVars) f(\reparamVars_{<i}, \observedVars)\\
%
&= \int_{\reparamVars_{<i}} \reparamDist(\reparamVars_{<i} | \observedVars) f(\reparamVars_{<i}, \observedVars) \int_{\reparamVar_i} \reparamDist(\reparamVar_i | \reparamVars_{<i}, \observedVars) \gradparams \log \reparamDist(\reparamVar_i | \reparamVars_{<i}, \observedVars) \int_{\reparamVars_{>i}} \reparamDist(\reparamVars_{>i} | \reparamVars_{\leq i}, \observedVars)\\
%
&= \int_{\reparamVars_{<i}} \reparamDist(\reparamVars_{<i} | \observedVars) f(\reparamVars_{<i}, \observedVars) \cdot \expect_{\reparamDist} [ \gradparams \log \reparamDist(\reparamVar_i | \reparamVars_{<i}, \observedVars) ] \cdot 1\\
%
&= \int_{\reparamVars_{<i}} \reparamDist(\reparamVars_{<i} | \observedVars) f(\reparamVars_{<i}, \observedVars) \cdot 0 = 0
\end{align*}
where the last line makes use of Lemma~\ref{lem:zeroexp}.
\end{proof}

\subsection{Variance Reduction Step 1: Zero Expectation $W$ Terms}

Related to BBVI Rao-blackwellization and NVIL-equivalent technique, etc.

\subsection{Variance Reduction Step 2: Baselines}

Next, we prove that subtracting a constant baseline term $b_i$ from every $w_i$ does not change the expectation in Equation~\ref{eq:finalEstimator}:
%%%
\begin{align*}
\expect_\reparamDist [ \gradparams \log \reparamDist(\reparamVar_i | \reparamVars_{<i}) (w_i(\reparamVars, \observedVars) - b_i) ]
%
&= \expect_\reparamDist [ \gradparams \log \reparamDist(\reparamVar_i | \reparamVars_{<i}) w_i(\reparamVars, \observedVars) ] - \expect_\reparamDist[ \gradparams \log \reparamDist(\reparamVar_i | \reparamVars_{<i}) b_i ]\\
%
&= \expect_\reparamDist [ \gradparams \log \reparamDist(\reparamVar_i | \reparamVars_{<i}) w_i(\reparamVars, \observedVars) ] 
\end{align*}
%%%
Where the last step makes use of Lemma~\ref{lem:zeroexp2}.

In our system, we use $b_i = \expect[ w_i ]$, which we estimate with of a moving average of the samples used to compute gradients. While this choice of $b_i$ is not guaranteed to reduce variance, it often does in practice, and previous systems for variational inference and reinforcement learning have exploited it~\cite{BBVI,StochasticComputationGraphs,VarianceReduction}. Another option is to \emph{learn} $b_i$, for example as a neural net function of $\observedVars$~\cite{NVIL}. The proof above also permits $b_i$ to be a function of $\reparamVars_{<i}$ (i.e. all previous random choices), which could reduce variance further by tracking posterior dependencies. This is a promising avenue for future work.

\subsection{Variance Reduction Step 3: Zero Expectation $q$ Factors}

Finally, we prove that we can remove any factors corresponding to discrete (i.e. non-reparameterized choices) from the $\gradparams \log \guide(\xformedVars | \observedVars)$ term in Equation~\ref{eq:hybridEstimator} without changing its expectation:
%%%
\begin{equation*}
\expect_\reparamDist [ \gradparams \log \guide(\reparamXform_i(\reparamVar_i) | \reparamXform_{<i}(\reparamVars_{<i}), \observedVars) ]
%
= \expect_\reparamDist [ \gradparams \log \reparamDist(\reparamVar_i | \reparamVars_{<i}, \observedVars) ]
%
= 0
\end{equation*}
where we have used Lemma~\ref{lem:zeroexp2} and the fact that $\reparamDist = \guide$ and $\reparamXform$ is the identity for discrete random choices.
%%%
