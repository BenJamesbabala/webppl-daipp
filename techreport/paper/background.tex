%auto-ignore
\section{Background}
\label{sec:background}

\subsection{Probabilistic Programming Basics}
\label{sec:pplbasics}

For our purposes, a probabilistic program defines a generative model $\trueJoint$ of latent variables $\latentVars$ and data $\observedVars$. The model factors as:
%%%
\begin{equation}
\trueJoint = p(\observedVars | \latentVars) \prod_i p(\latentVar_i | \latentVars_{<i})
\label{eq:probProgDef}
\end{equation}
%%%
The prior probability distribution $p(\latentVars)$ decomposes as a product of conditionals $p_i(\latentVar_i | \latentVars_{<i})$, one for each random choice $\latentVar_i$ in the program. The use of $\latentVars_{<i}$ indicates that a random choice can potentially depend on any or all previous choices made by the program.
$p(\observedVars | \latentVars)$ is the likelihood of the data and need not be a proper probability distribution (i.e. unnormalized factors are acceptable).
Note that $\latentVars$ can vary in length across executions: a probabilistic program can sample a variable number of random variables.

Our system is implemented in the probabilistic programming language WebPPL, which we use for examples throughout this paper~\cite{WebPPL}.
WebPPL is a PPL embedded in Javascript; that is, it adds sampling, conditioning, and inference operators to a purely-functional subset of Javascript.
The following example program illustrates its basic features:
\begin{lstlisting}
var model = function() {
   var x = sample(Bernoulli({p: 0.75}));
   var mu = x ? 2 : 0;
   observe(Gaussian({mu: mu, sigma: 1}), 0.5);
   return x;
};

Infer({method: 'MCMC'}, model);
\end{lstlisting}
This program uses MCMC to compute an approximate posterior distribution over the return value of the function \ic{model}. \ic{model} is a simple generative model with one latent Bernoulli variable (\ic{x}) and one observed Gaussian variable, which in this example is observed to have the value \ic{0.5}. The mean of the observed Gaussian variable (\ic{mu}) is dependent on the value of \ic{x}. Since \ic{model} returns \ic{x}, the result of this program is the posterior marginal distribution over the variable \ic{x}.
In the rest of this paper, we will build on this language, adding guide programs, amortized inference, and model-learning constructs.

\subsection{Inference as Optimization: Variational Inference}
\label{sec:background:variational}

Instead of approximating the posterior $\truePosterior$ with a collection of samples, one could instead try to approximate it via a parameterized distribution $\guide_{\observedVars}(\latentVars ; \phi)$ which is itself easy to sample from.
This is the premise behind variational inference~\cite{VariationalInference}.
The goal is to find parameters $\phi$ such that $\guide_{\observedVars}(\latentVars ; \phi)$ is as close as possible to $\truePosterior$, where closeness is typically measured via KL-divergence.

To use variational inference, one must first choose a parameterized family of distributions $\guide$; one common choice is the \emph{mean-field family}:
%%%
\begin{equation*}
\guide^{\textbf{MF}}_{\observedVars}(\latentVars ; \phi) = \prod_i \guide(\latentVar_i ; \phi_i)
\end{equation*}
%%%
This is a fully-factored distribution: it approximates the true posterior as an independent product of parameterized marginals, one for each random variable.
Several existing general-purpose variational inference systems use this scheme~\cite{AVIPP,BBVI}.
This is easy to work with, but it does not capture any of the dependencies between variables that may occur in the true posterior.
This limitation is often acceptable because $\guide_{\observedVars}$ is defined relative to a particular observation set $\observedVars$, and thus the parameters are re-optimized for each new $\observedVars$.
Thus, while this scheme provides an alternative to Monte Carlo methods (e.g. MCMC) that can be faster and more reliable, it still solves each inference problem from scratch.

\subsection{Amortized (Variational) Inference}

As mentioned in Section~\ref{sec:introduction}, \emph{amortized inference} refers to the use of previous inference solutions (or other pre-computation) to solve subsequent inference problems faster.
There exists experimental evidence that people leverage experience from prior inference tasks when asked to solve related ones~\cite{AmortizedInference}.
This idea has inspired research into developing amortized inference systems for Bayesian networks~\cite{StochasticInverses,NeuralStochasticInverses}. These systems model $p(\latentVars | \observedVars)$ by inverting the network topology and attempting to learn the local conditional distributions of this inverted graphical model.

Amortized inference can also be achieved through variational inference.
Instead of defining a parametric family $\guide_{\observedVars}(\latentVars ; \phi)$ which is specific to a given $\observedVars$, we can instead define a general family $\guidePosterior$ which is conditional on $\observedVars$; that is, it takes $\observedVars$ as input.
In this setting, the mean field family no longer applies, as the factors of $\guide$ must now be functions of $\observedVars$.
However, we can extend the mean field family to handle input data by using neural networks (or other `side computations'):
%%%
\begin{equation*}
\guidePosterior = \prod_i \guide(\latentVar_i ; \text{NN}_i(\observedVars ; \phi))
\end{equation*}
%%%
Here, the parameters of each local conditional in the guide $\guide$ are computed via a neural network function $\text{NN}_i$ of $\observedVars$.
This variational family supports amortized inference: one can invest time up-front optimizing the neural network weights such that $\guidePosterior$ is close to $\truePosterior$. When given a never-before-seen $\observedVars$, the guide program forwards $\observedVars$ through the trained networks for fast inference.
Several recent approaches to `neural variational inference' use some instantiation of this design pattern~\cite{NVIL,DLGM,AEVB}.

Such neural guide families are easy to express in our extensions of WebPPL. Our system also allows generalizations of this pattern, such as providing neural nets with previously-made random choices as additional input:
%%%
\begin{equation*}
\guidePosterior = \prod_i \guide(x_i ; \text{NN}_i(\observedVars, \latentVars_{<i} ; \phi))
\end{equation*}
%%%
Here, $\latentVars_{<i}$ are the random choices made before choice $i$ is sampled. Such guide families have the potential to capture posterior dependencies between latent variables
% ~\remark{Which we demonstrate later in the paper}.

\subsection{Variational Model Learning}

The amortized variational inference framework presented above can also be used to learn the parameters of the generative model $p$. If $p$ is also parameterized, i.e. $\truePosteriorTheta$, then its parameters $\theta$ can be optimized along with the parameters $\phi$ of the guide program~\cite{NVIL,DLGM,AEVB}.

Our system supports learning generative model parameters in addition to guide parameters.
In the PPL setting it is natural to think of this as a particular model pattern in which there are global parameters or random choices that affect a local `observation model', which in turn is assumed to generate each data point independently; we call this the mapData model pattern.
We will show below how it is easy to use this pattern to do (regularized) maximum-likelihood learning, variational Bayes, or even different methods within the same model.

