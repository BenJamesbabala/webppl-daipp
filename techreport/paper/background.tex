%auto-ignore
\section{Background}
\label{sec:background}

\subsection{Probabilistic Programming Basics}
\label{sec:pplbasics}

For our purposes, a probabilistic program defines a generative model $\trueJoint$ of latent variables $\latentVars$ and data $\observedVars$. The model factors as:
%%%
\begin{equation}
\trueJoint = p(\observedVars | \latentVars) \prod_i p_i(\latentVar_i | \latentVars_{<i})
\label{eq:probProgDef}
\end{equation}
%%%
\remark{Is this ok, or does the variance reduction stuff later require likelihood to be factored, too?}
The prior probability distribution $p(\latentVars)$ decomposes as a product of conditionals $p_i(\latentVar_i | \latentVars_{<i})$, one for each random choice $\latentVar_i$ in the program. The use of $\latentVars_{<i}$ indicates that a random choice can potentially depend on any or all previous choices made by the program.
$p(\observedVars | \latentVars)$ is the likelihood of the data and need not be a proper probability distribution (i.e. unnormalized factors are acceptable).
Note that $\latentVars$ can vary in length across executions: a probabilistic program can sample a variable number of random variables.

Our system is implemented in the probabilistic programming language WebPPL, which we'll use for examples throughout this paper~\cite{WebPPL}.
WebPPL is a PPL embedded in Javascript.
That is, it adds sampling, condition, and inference operators to a purely-functional subset of JS.
Here's an example program illustrating its basic features:
\begin{lstlisting}
var model = function() {
   var x = sample(Bernoulli({p: 0.75}));
   var mu = x ? 2 : 0;
   observe(Gaussian({mu: mu, sigma: 1}), 0.5);
   return x;
};

Infer({method: 'MCMC'}, model);
\end{lstlisting}
This program uses MCMC to compute an approximate posterior distribution over the return value of the function \ic{model}. \ic{model} is a simple generative model with one latent Bernoulli variable (\ic{x}) and one observed Gaussian variable, which in this example is observed to have the value \ic{0.5}. The mean of the observed Gaussian variable (\ic{mu}) is dependent on the value of \ic{x}. Since \ic{model} returns \ic{x}, the result of this program is the posterior marginal distribution over the variable \ic{x}.
\remark{Note that \ic{observe} is built on the more general \ic{factor}?}
In the rest of this paper, we will build on this language to add guide programs and amortized inference to it.

\subsection{Inference as Optimization: Variational Inference}
\label{sec:background:variational}

Instead of approximating the posterior $\truePosterior$ with a collection of samples, could instead try to approximate it via a parameterized distribution $\guide_{\observedVars}(\latentVars ; \phi)$ which is itself easy to sample from.
This is what variational inference is all about~\cite{VariationalInference}.
The goal is to find parameters $\phi$ such that $\guide_{\observedVars}(\latentVars ; \phi)$ is as close as possible to $\truePosterior$, where closeness is typically measured via KL-divergence.

Need to pick a parametrized family $\guide$ to do this; one common choice is the \emph{mean-field family}:
%%%
\begin{equation*}
\guide^{\textbf{MF}}_{\observedVars}(\latentVars ; \phi) = \prod_i g_i(\latentVar_i ; \phi_i)
\end{equation*}
%%%
This is a fully-factored distribution: it approximates the posterior as an independent product of parameterized marginals $\guide_i$, one for each random variable.
Several existing general-purpose variational inference systems use this scheme~\cite{AVIPP,BBVI}.
This is easy to work with, but it does not capture any of the correlations/dependencies in the true posterior.
This limitation is often acceptable b/c $\guide_{\observedVars}$ is defined to be specific to a particular $\observedVars$, and thus the parameters are re-optimized for each new $\observedVars$.
So this gives us an alternative to Monte Carlo methods (e.g. MCMC) that can be faster and more reliable, but it is still solving inference problems from scratch each time.

\subsection{Amortized (Variational) Inference}

Remind what amortized inference is: using results of previous inference solutions, or pre-computation in general, to solve later inference problems faster.
Evidence that people do this~\cite{AmortizedInference}.
Some previous work tries to implement this principle for Bayesian networks, by inverting the network topology and learning approximations to the local inverse conditional distributions~\cite{StochasticInverses,NeuralStochasticInverses}.

We can also view amortized inference as an extention to the variational inference idea.
Instead of defining a parametric family $\guide_{\observedVars}(\latentVars ; \phi)$ which is specific to a given $\observedVars$, we instead define a general family $\guidePosterior$ which is conditional on $\observedVars$; that is, it takes $\observedVars$ as input.
In this setting, mean field no longer applies, as whatever components are used to make up $\guide$ must now be functions of $\observedVars$.
Can extend mean-field to deal with input data by using neural networks:
%%%
\begin{equation*}
\guidePosterior = \prod_i \guide_i(\latentVar_i ; \text{NN}_i(\observedVars ; \phi))
\end{equation*}
%%%
That is, the parameters of each local conditional in the guide are computed via a neural network function of $\observedVars$.
This is amortized inference b/c you can invest time up-front optimizing the weights of these neural networks such that $\guidePosterior$ is close to $\truePosterior$, and the trained nets can then be used on never-before-seen $\observedVars$'s.
Several recent approaches to `neural variational inference' use some instantiation of this design pattern~\cite{NVIL,DLGM,AEVB}.

Our system also takes this general approach using neural nets but is more general; it allows neural nets also to receive as input any random choices already made:
%%%
\begin{equation*}
\guidePosterior = \prod_i \guide_i(x_i ; \text{NN}_i(\observedVars, \latentVars_{<i} ; \phi))
\end{equation*}
%%%
where $\latentVars_{<i}$ are the random choices made before choice $i$ is sampled. This allows the networks to capture posterior dependencies between latent variables (which we will demonstrate later in the paper).

\subsection{Variational Model Learning}

This same amortized variational inference setup can also be used to learn the parameters of generative models. If the generative model $p$ is also parameterized, i.e. $\truePosteriorTheta$, then its parameters $\theta$ can be optimized along with the parameters $\phi$ of the guide program.
The `neural variational inference' methods mentioned above can all do this~\cite{NVIL,DLGM,AEVB}.

Our system also supports learning generative model parameters in addition to guide parameters.
We will show examples of how our system makes it easy to use either (regularized) maximum-likelihood learning or full variational Bayesian learning, or to switch between them.

