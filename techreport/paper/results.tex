%auto-ignore
\section{Experiments}
\label{sec:results}

Having detailed how to specify and optimize guide programs in our system, in this section, we experimentally evaluate how well programs written in our system can learn generative models and approximate posterior samplers. Unless stated otherwise, we use the following settings for the experiments in this section:
%%%
\begin{itemize}
\item{The Adam optimization method~\cite{Adam} with $\alpha = 0.1$, $\beta_1 = 0.9$, and $\beta_2 = 0.99$.}
\item{One sample from $\guide$ per optimization step to estimate the expectation in Equation~\ref{eq:finalEstimator}.}
\end{itemize}

\subsection{Gaussian Mixture Model}
\label{sec:results_gmm}

We first consider the simple Gaussian mixture model program from Figure~\ref{fig:bn_oneLatent} Bottom. This program samples discrete random choices, so its gradient estimator will include an LR term. Alternatively, we could re-write the program slightly to explicitly marginalize out the discrete random choices; see Appendix~\ref{sec:appendix_code:gmmSumOut}. The gradient estimator for this program then reduces to the PW estimator, which will have lower variance. Lower variance comes at the cost of amortized inference, however, as this version of the program does not have a guide which can predict the latent cluster assignment given an observed point. We also consider a non-amortized, mean field version of the program for comparison.

Figure~\ref{fig:gmmResults} illustrates the performance of these programs after training for 200 steps on a synthetic datset of 100 points. We learn model parameters using regularized maximum likelihood estimation. On the left, we show how the ELBo changes during optimizaton. Even on a simple model such as this one, the ability to use the pure PW estimator leads to faster convergence and a better final objective score.
On the right, we show the estimated negative log likelihood of a separate synthetic test set under each program after optimization. Here, we also include the tue model (i.e. the model used to synthesize the test/training data) for comparsion.
As suggested by its optimization performance, the model with discrete choices marginalized out performs best.
Note that the amortized guide program slightly out-performs the mean field guide program, indicating that the generalization provided by amortization has benefits for training generative models, in addition to enabling fast predictions of latent variables for previously-unseen observations.

\begin{figure}[!ht]
\begin{minipage}{0.5\linewidth}
\centering
\includegraphics[width=\linewidth]{figs/results/gmm/elboProgress.pdf}
\end{minipage}
%
\begin{minipage}{0.5\linewidth}
\centering
\includegraphics[width=\linewidth]{figs/results/gmm/nll.pdf}
\end{minipage}
\caption{Performance of simple Gaussian mixture model program. \emph{(Left)} ELBo optimization progress during training. \emph{(Right)} Negative log-likelihood of a held-out test set.}
\label{fig:gmmResults}
\end{figure}

% Other Bayes net examples??


\subsection{Neural Generative Models: Variational Autoencoder \& Sigmoid Belief Network}
\label{sec:results_vae}

Our system naturally supports generative models which use neural network components. Two prominent examples of models in this class include the Variational Autoencoder (VAE) and Sigmoid Belief Networks (SBN). Both models samples latent variables from a multivariate distribution and then transform the result via a neural network to produce observed variables, often in the form of an image. The VAE uses a latent multivariate Gaussian distribution, whereas the SBN uses a latent multivariate Bernoulli.

Appendix~\ref{sec:appendix_code} shows implementations of these models in our system~\remark{Actually do this}. Our VAE implementation follows the original description of the model by Kingma and Welling~\cite{AEVB}, and our SBN implementation follows that of Mnih and Gregor~\cite{NVIL}.
The VAE uses a 20-dimensional latent code, and the SBN uses a single layer of 200 hidden variables. Our system cannot express the two-layer SBN of Mnih and Gregor, because its guide model samples the latent variables in reverse order from the generative model.

Figure~\ref{fig:vae_sbn_results} Left shows results of training these models on the MNIST dataset, using Adam with a step size of 0.001.
While both models train quickly at first, the SBN's training slows more noticeably than the VAE's due to its discrete nature. It takes more than three times as many iterations to achieve the same ELBo.
In Figure~\ref{fig:vae_sbn_results} Right, we qualitatively evaluate both models by using them to reconstruct images from the MNIST test set. We use the guide program to sample latent variables conditional on the images in the ``Target'' column (i.e. the `encoder' phase). We then transform these latent variables using the generative model's neural networks (i.e. the `decoder' phase) to produce the reconstructed images in the ``VAE'' and ``SBN'' columns.
As suggested by their training behavior, the VAE is able to generate higher-quality reconstructions after less training.

Our optimization exhibits some differences from the previous work.
For the VAE, Kingma and Welling~\cite{AEVB} exploit the closed form solution of the KL divergence between two Gaussians to create an even lower-variance estimator of the ELBo gradient. We use a more general formulation, but our system can still successfully train the model.
For the SBN, Mnih and Gregor~\cite{NVIL} use neural networks to compute the per-variable baselines $b_i$ in Equation~\ref{eq:finalEstimator}, whereas we use a simpler approach (see Appendix~\ref{sec:appendix_proofs}).

\begin{figure}
\begin{minipage}{0.5\linewidth}
\centering
\includegraphics[width=\linewidth]{figs/results/vae_sbn/elboProgress.pdf}
\end{minipage}
%
\hspace{2em}
%
\begin{minipage}{0.5\linewidth}
\setlength{\tabcolsep}{1pt}
\centering
\begin{tabular}{c  c c c c c c}
Target & \multicolumn{3}{c}{VAE} & \multicolumn{3}{c}{SBN}
\\
 \includegraphics[width=0.12\linewidth]{figs/results/vae_sbn/vae_encodeDecode_target_000.png}
 \hspace{3pt}
& \includegraphics[width=0.12\linewidth]{figs/results/vae_sbn/vae_encodeDecode_target_000_sample_001.png}
& \includegraphics[width=0.12\linewidth]{figs/results/vae_sbn/vae_encodeDecode_target_000_sample_002.png}
& \includegraphics[width=0.12\linewidth]{figs/results/vae_sbn/vae_encodeDecode_target_000_sample_003.png}
\hspace{3pt}
& \includegraphics[width=0.12\linewidth]{figs/results/vae_sbn/sbn_encodeDecode_target_000_sample_001.png}
& \includegraphics[width=0.12\linewidth]{figs/results/vae_sbn/sbn_encodeDecode_target_000_sample_002.png}
& \includegraphics[width=0.12\linewidth]{figs/results/vae_sbn/sbn_encodeDecode_target_000_sample_003.png}
\\
 \includegraphics[width=0.12\linewidth]{figs/results/vae_sbn/vae_encodeDecode_target_001.png}
 \hspace{3pt}
& \includegraphics[width=0.12\linewidth]{figs/results/vae_sbn/vae_encodeDecode_target_001_sample_001.png}
& \includegraphics[width=0.12\linewidth]{figs/results/vae_sbn/vae_encodeDecode_target_001_sample_002.png}
& \includegraphics[width=0.12\linewidth]{figs/results/vae_sbn/vae_encodeDecode_target_001_sample_003.png}
\hspace{3pt}
& \includegraphics[width=0.12\linewidth]{figs/results/vae_sbn/sbn_encodeDecode_target_001_sample_001.png}
& \includegraphics[width=0.12\linewidth]{figs/results/vae_sbn/sbn_encodeDecode_target_001_sample_002.png}
& \includegraphics[width=0.12\linewidth]{figs/results/vae_sbn/sbn_encodeDecode_target_001_sample_003.png}
\\
 \includegraphics[width=0.12\linewidth]{figs/results/vae_sbn/vae_encodeDecode_target_002.png}
 \hspace{3pt}
& \includegraphics[width=0.12\linewidth]{figs/results/vae_sbn/vae_encodeDecode_target_002_sample_001.png}
& \includegraphics[width=0.12\linewidth]{figs/results/vae_sbn/vae_encodeDecode_target_002_sample_002.png}
& \includegraphics[width=0.12\linewidth]{figs/results/vae_sbn/vae_encodeDecode_target_002_sample_003.png}
\hspace{3pt}
& \includegraphics[width=0.12\linewidth]{figs/results/vae_sbn/sbn_encodeDecode_target_002_sample_001.png}
& \includegraphics[width=0.12\linewidth]{figs/results/vae_sbn/sbn_encodeDecode_target_002_sample_002.png}
& \includegraphics[width=0.12\linewidth]{figs/results/vae_sbn/sbn_encodeDecode_target_002_sample_003.png}
\\
 \includegraphics[width=0.12\linewidth]{figs/results/vae_sbn/vae_encodeDecode_target_003.png}
 \hspace{3pt}
& \includegraphics[width=0.12\linewidth]{figs/results/vae_sbn/vae_encodeDecode_target_003_sample_001.png}
& \includegraphics[width=0.12\linewidth]{figs/results/vae_sbn/vae_encodeDecode_target_003_sample_002.png}
& \includegraphics[width=0.12\linewidth]{figs/results/vae_sbn/vae_encodeDecode_target_003_sample_003.png}
\hspace{3pt}
& \includegraphics[width=0.12\linewidth]{figs/results/vae_sbn/sbn_encodeDecode_target_003_sample_001.png}
& \includegraphics[width=0.12\linewidth]{figs/results/vae_sbn/sbn_encodeDecode_target_003_sample_002.png}
& \includegraphics[width=0.12\linewidth]{figs/results/vae_sbn/sbn_encodeDecode_target_003_sample_003.png}
\end{tabular}
\end{minipage}
\caption{Evaluating the Variational Autoencoder (VAE) and Sigmoid Belief Network (SBN) programs on the MNIST dataset. \emph{(Left)} ELBo optimization progress during training. \emph{(Right)} Reconstructing the images in the ``Target'' column using both models.}
\label{fig:vae_sbn_results}
\end{figure}


\subsection{Latent Dirichlet Allocation}
\label{sec:results_lda}

Introduce the CoCoLab abstracts dataset (is there a citation for this yet?)

Just (manually-specified) mean-field guides, for now.

Show Top N words for each learned topic?


Things to try if we have time:
\begin{itemize}
\item{Simple bayesian NN (show how easy it is to do full variational Bayes)}
\item{HMM-type families: Deep kalman filter? Or other RNN VAE type thing?}
\item{PCFG (prob wonâ€™t work, but useful to understand). Continuous feature-passing version?}
\end{itemize}

