%auto-ignore
\section{Deriving Guides Automatically}
\label{sec:autoGuide}

Thus far, we have shown how we can succesfully create and train guide programs for several types of generative models. However, writing guide programs can sometimes be tedious and repetitive; for example, note the large amount of shared structure between the guides shown in Figures~\ref{fig:bn_oneLatent} and \ref{fig:bn_twoLatent}. Furthermore, it is not always obvious how to write a good guide program. In Figure~\ref{fig:bn_twoLatent}, knowledge of the structure of this very simple generative model led us to add a direct dependency between the two latent variables in the guide. For general programs---especially large, complex ones---it will not always be clear what these dependencies are or how to capture them with a guide.

This section describes our early experience with automatically deriving guide programs. We first describe how our system provides sensible default behavior that can make writing some guides less cumbersome. We then outline how the system might be extended to automatically derive guides for any program using recurrent neural networks.

\subsection{Mean Field by Default}
\label{sec:autoGuide:meanField}

If a call to \ic{sample} is not provided with an explicit guide distribution, our system automatically inserts a mean field guide. For example, the code \ic{sample(Gaussian(\{mu: 0, sigma: 1\}))} results in:
%%%
\begin{lstlisting}
sample(Gaussian({mu: 0, sigma: 1}), {
   guide: Gaussian({mu: param({name: <@\hilite{<auto_name>}@>}), sigma: softplus(param({name: <@\hilite{<auto_name>}@>}))})
})
\end{lstlisting}
%%%
where parameter bounding transforms such as \ic{softplus} are applied based on bounds metadata provided with each primitive distribution type. We use reparameterizable guides for continuous distributions (see Appendix~\ref{sec:appendix_reparam}).

Since this process declares new optimizable parameters automatically, we must automatically generate names for these parameters. Our system names parameters according to where they are declared in the program execution trace, using the same naming technique as is used for random choices in probabilistic programming MCMC engines~\cite{Lightweight}. Since the names of these parameters are tied to the structure of the program, they cannot be re-used by other programs (as in the `Further Optimization' example of Section~\ref{sec:furtherOptim}).

\subsection{Beyond Mean Field: Automatic Factored Guides with Recurrent Networks}

In Section~\ref{sec:results_qmr}, we experimented with a factored guide program for the QMRâ€“DT model. We think that this general style of guide---predicting each random choice in sequence, conditional on the hidden state of a recurrent neural network---might be generalized to an automatic guide for any program, as any probabilistic program can be decomposed into a sequence of random choices. In our QMR-DT experiments, we used a separate neural network (with separate parameters) to predict each latent variable (i.e. random choice). For complex models and large data sets, this approach would lead to a computationally unfeasible explosion in the number of parameters. Furthermore, it is likely that the prediction computations for many random choices in the program are related. For example, in the QMR-DT program, latent causes that share many dependent effects may be well predicted by the same or very similar networks.

Given these insights, we imagine a universally-applicable guide that uses a single prediction network for all random choices, but to which each random choice provides an additional identifying input. These IDs should be elements in a vector space, such that more `similar' random choices have IDs which are close to one another for some distance metric in the vector space. One possible way to obtain such IDs would be to learn an embedding of the program-structural addresses of each random choice~\cite{Lightweight}. These might be learned in an end-to-end fashion by making them learnable parameter vectors in the overall variational optimization (i.e. letting closeness in the embedding space be an emergent property of optimizing our overall objective).

% \subsection{Capturing Posterior Dependencies with Context Nets}

% Context nets, val2vec, and vec2dist.

% When to init context (e.g. incorporating datum in each mapData iteration vs. something more global).

% Architecture choices: LSTM, bilinear resnets, linear predict net, etc.

% If we have time:
% \begin{itemize}
% \item{Context nets with long distance connections?}
% \item{Automatic mixtures for continuous distributions?}
% \item{Other interesting guides?}
% \end{itemize}


