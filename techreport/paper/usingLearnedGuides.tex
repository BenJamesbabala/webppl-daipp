%auto-ignore
\section{Using Learned Parameters}
\label{sec:usingLearnedGuides}

Given a set of learned parameters, our system can predict latent variables, generate synthetic data, or further refine parameters for a new dataset or a new model.

\subsection{Predicting Latent Variables}

A learned guide can be used to make inferences about new, never-before-seen observations. As an example, we'll use the Gaussian mixture model program in Figure~\ref{fig:bn_oneLatent} Bottom and show how to predict cluster assignments for new observations. Note that the observations used by the program are assigned to the \ic{obs} variable, which is then passed to \ic{mapData}. WebPPL is a purely functional language, so it does not support assigning a new dataset to \ic{obs}. However, it does provide a special \ic{globalStore} object whose fields can be re-assigned. With this in mind, we modify the Gaussian mixture model program as follows\footnote{An alternative to using \ic{globalStore} for mutation would be to re-create the \ic{model} function, closing over the test data instead of the training data.}:
%%%
\begin{lstlisting}
globalStore.data = loadData('data.json');
// Set up guide neural net
var model = function() {
   // Set up generative parameters
   var latents = mapData({data: globalStore.data}, function(y) {
      // Guided sample latents, observe data points
   });
   return latents;
};
\end{lstlisting}
%%%
Now we can easily swap datasets using \ic{globalStore.data}. Given a set of learned parameters \ic{params} for this program, we can obtain a sample prediction for the latent variables for a new dataset:
%%%
\begin{lstlisting}
globalStore.data = loadData('data_test.json');	// Load new test data set

// Forward sample from the guide
sample(Infer({method: 'forward', guide: true, params: params}, model));

// Use the guide as a Sequential Monte Carlo importance sampler
sample(Infer({method: 'SMC', particles: 100, params: params}));
\end{lstlisting}
%%%
We can make predictions either by running the guide program forward, or if the true posterior is very complex and the learned guide only partially approximates it, we can use the guide program as an importance sampler within Sequential Monte Carlo.

\subsection{Generating Synthetic Data}

Forward sampling from the guide can also be used to generate synthetic data from the learned distribution. If we make a slightly modified version of the Gaussian mixture model (call it \ic{modelGen}) that samples data instead of observing it, we can used forward sampling with the optimized parameters \ic{params} to synthesize new data points:
%%%
\begin{lstlisting}
var modelGen = function() {
   var theta_x = simplex(modelParam({dims: [nComps-1, 1], name: 'theta_x'}));
   var params_y = [
      {mu: modelParam({name: 'mu1'}), sigma: softplus(modelParam({name: 's1'}))},
      {mu: modelParam({name: 'mu2'}), sigma: softplus(modelParam({name: 's2'}))},
      {mu: modelParam({name: 'mu3'}), sigma: softplus(modelParam({name: 's3'}))}
   ];
   var x = sample(Discrete({ps: theta_x}));
   return sample(Gaussian(params_y[x]));
};

sample(Infer({method: 'forward', guide: true, params: params}, modelGen));
\end{lstlisting}

\subsection{Further Optimization}
\label{sec:furtherOptim}

A set of learned parameters \ic{params} can also be passed back into \ic{Optimize} for further optimization:
%%%
\begin{lstlisting}
var newParams = Optimize(model, {
   steps: 100,
   optMethod: 'adam',
   params: params
});
\end{lstlisting}
%%%
This can be useful for e.g. fine-tuning existing parameters for a new dataset. Indeed, \ic{model} does not even need to be the same program that was originally used to learn \ic{params}; it just needs to declare some parameters (via \ic{param}) with the same names as parameters in \ic{params}. This can be useful for, for example, making a modification to an existing model without having to re-train its guide program from scratch, or for bootstrap training from a simpler model to a more complex one.
