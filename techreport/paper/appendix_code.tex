%auto-ignore

\section{Appendix: Example Programs}
\label{sec:appendix_code}

\subsection{Gaussian mixture model}
\label{sec:appendix_code:gmmSumOut}

With discrete choices marginalized out:
\begin{lstlisting}
var obs = loadData('data.json');
var nComps = 3
var model = function() {
   var theta_x = simplex(modelParam({dims: [nComps-1, 1], name: 'theta_x'}));
   var params_y = [
      {mu: modelParam({name: 'mu1'}), sigma: softplus(modelParam({name: 's1'}))},
      {mu: modelParam({name: 'mu2'}), sigma: softplus(modelParam({name: 's2'}))},
      {mu: modelParam({name: 'mu3'}), sigma: softplus(modelParam({name: 's3'}))}
   ];
   mapData({data: obs}, function(y) {
      // Explicitly sum out latent mixture component
      var scores = mapIndexed(function(i, muSigma) {
         var w = T.get(theta_x, i);
         return Gaussian(muSigma).score(y) + Math.log(w);
      }, params_y);
      // Summed-out likelihod
      factor(logsumexp(scores));
   });
};
\end{lstlisting}

\subsection{QMR-DT}
\label{sec:appendix_code:qmr}

Program with joint guide:
%%%
\begin{lstlisting}
var graph = loadQMRGraph('qmr_graph.json');
var data = loadData('qmr_data.json');

var noisyOrProb = function(symptomNode, diseases) {
   var cp = product(map(function(parent) {
      return diseases[parent.index] ? (1 - parent.prob) : 1;
   }, symptomNode.parents));
   return 1 - (1-symptomNode.leakProb)*cp;
};

var guideNet = nn.mlp(graph.numSymptoms, [
   {nOut: graph.numDiseases, activation: sigmoid}
], 'guideNet');
var predictDiseaseProbs = function(symptoms) {
   return nneval(guideNet, Vector(symptoms));
};

var model = function() {
   mapData({data: data, batchSize: 20}, function(symptoms) {
      var predictedProbs = predictDiseaseProbs(symptoms);
      var diseases = mapIndexed(function(i, disease) {
         return sample(Bernoulli({p: disease.priorProb}), {
            guide: Bernoulli({p: T.get(predictedProbs, i)})
         });
      }, graph.diseaseNodes);

      mapData({data: symptoms}, function(symptom, symptomIndex) {
         var symptomNode = graph.symptomNodes[symptomIndex];
         observe(Bernoulli({p: noisyOrProb(symptomNode, diseases)}), symptom);
      });
   });
};
\end{lstlisting}

Program with factored guide. Note that the guide uses a separate neural network (with separate parameters) to predict each latent cause.
%%%
\begin{lstlisting}
var graph = loadQMRGraph('qmr_graph.json');
var data = loadData('qmr_data.json');

var noisyOrProb = function(symptomNode, diseases) {
   var cp = product(map(function(parent) {
      return diseases[parent.index] ? (1 - parent.prob) : 1;
   }, symptomNode.parents));
   return 1 - (1-symptomNode.leakProb)*cp;
};

var predictNet = cache(function(i) {
   return nn.mlp(graph.numSymptoms, [
      {nOut: 1, activation: sigmoid}
   ], 'predictNet_'+i);
});
var predictDiseaseProb = function(symptoms, i) {
   return T.get(nneval(predictNet(i), Vector(symptoms)), 0);
};

var model = function() {
   mapData({data: data, batchSize: 20}, function(symptoms) {
      var diseases = mapIndexed(function(i, disease) {
         return sample(Bernoulli({p: disease.priorProb}), {
            guide: Bernoulli({p: predictDiseaseProb(symptoms, i)})
         });
      }, graph.diseaseNodes);

      mapData({data: symptoms}, function(symptom, symptomIndex) {
         var symptomNode = graph.symptomNodes[symptomIndex];
         observe(Bernoulli({p: noisyOrProb(symptomNode, diseases)}), symptom);
      });
   });
};
\end{lstlisting}

Program with factored guide with GRU.
%%%
\begin{lstlisting}
var graph = loadQMRGraph('qmr_graph.json');
var data = loadData('qmr_data.json');
var gruHiddenDim = 20;

var noisyOrProb = function(symptomNode, diseases) {
   var cp = product(map(function(parent) {
      return diseases[parent.index] ? (1 - parent.prob) : 1;
   }, symptomNode.parents));
   return 1 - (1-symptomNode.leakProb)*cp;
};

var predictNet = cache(function(i) {
   return nn.mlp(graph.numSymptoms + gruHiddenDim, [
      {nOut: 1, activation: sigmoid}
   ], 'predictNet_'+i);
});
var predictDiseaseProb = function(symptoms, i) {
   var inputs = T.concat(Vector(symptoms), globalStore.gruHiddenState);
   return T.get(nneval(predictNet(i), inputs, 0);
};

// GRU cell: takes input + previous hidden state, produces new hidden state.
var gru = makeGRU(gruHiddenDim, 1, 'gru');
var updateGRUState = function(latentChoiceVal) {
   var inputs = T.concat(Vector([latentChoiceVal]), globalStore.gruHiddenState);
   globalStore.gruHiddenState = nneval(gru, inputs);
};

var model = function() {
   mapData({data: data, batchSize: 20}, function(symptoms) {
      globalStore.gruHiddenState = zeros([gruHiddenDim]);
      var diseases = mapIndexed(function(i, disease) {
         var x = sample(Bernoulli({p: disease.priorProb}), {
            guide: Bernoulli({p: predictDiseaseProb(symptoms, i)})
         });
         updateGRUState(x);
         return x;
      }, graph.diseaseNodes);

      mapData({data: symptoms}, function(symptom, symptomIndex) {
         var symptomNode = graph.symptomNodes[symptomIndex];
         observe(Bernoulli({p: noisyOrProb(symptomNode, diseases)}), symptom);
      });
   });
};
\end{lstlisting}

\subsection{LDA}
\label{sec:appendix_code:lda}

To simplify the programs in this section, we use the technique of Section~\ref{sec:autoGuide:meanField} to insert a mean field \ic{LogisticNormal} guide for any \ic{Dirichlet} random choice that does not have an explicit guide declared.

Mean field model:
\begin{lstlisting}
var model = function(corpus, vocabSize, numTopics, alpha, eta) {
  var topics = repeat(numTopics, function() {
    return sample(Dirichlet({alpha: eta}));
  });

  mapData({data: corpus}, function(doc) {

    var topicDist = sample(Dirichlet({alpha: alpha}));

    mapData({data: doc}, function(word) {
      var z = sample(Discrete({ps: topicDist}));
      var topic = topics[z];
      observe(Discrete({ps: topic}), word);
    });

  });

  return topics;
};
\end{lstlisting}

Marginalized mean field model:
\begin{lstlisting}
var model = function(corpus, vocabSize, numTopics, alpha, eta) {

  var topics = repeat(numTopics, function() {
    return sample(Dirichlet({alpha: eta}));
  });

  mapData({data: corpus}, function(doc) {

    var topicDist = sample(Dirichlet({alpha: alpha}));

    forEach(doc, function(count, word) {

      if (count > 0) {
        // Sum over topic assignments/z.
        var prob = sum(mapN(function(z) {
          var zScore = Discrete({ps: topicDist}).score(z);
          var wgivenzScore = Discrete({ps: topics[z]}).score(word);
          return Math.exp(zScore + wgivenzScore);
        }, numTopics));

        factor(Math.log(prob) * count);
      }

    });

  });

  return topics;

};
\end{lstlisting}

Word-level guide:
\begin{lstlisting}
var model = function(corpus, vocabSize, numTopics, alpha, eta) {

  var numHid = 50;
  var embedSize = 50;
  var embedNet = nn.mlp(vocabSize, [{nOut: embedSize, activation: nn.tanh}], 'embedNet');

  var net = nn.mlp(embedSize + numTopics, [
    {nOut: numHid, activation: nn.tanh},
    {nOut: numTopics}
  ], 'net');

  var wordAndTopicDistToParams = function(word, topicDist) {
    var embedding = nneval(embedNet, oneOfK(word, vocabSize));
    var out = nneval(net, T.concat(embedding, T.sub(topicDist, 1)));
    return {ps: softplus(tensorToVector(out))};
  };

  var topics = repeat(numTopics, function() {
    return sample(Dirichlet({alpha: eta}));
  });

  mapData({data: corpus}, function(doc) {

    var topicDist = sample(Dirichlet({alpha: alpha}));

    mapData({data: doc}, function(word) {
      var z = sample(Discrete({ps: topicDist}), {
        guide: Discrete(wordAndTopicDistToParams(word, topicDist))
      });
      var topic = topics[z];
      observe(Discrete({ps: topic}), word);
    });

  });

  return topics;
};
\end{lstlisting}

Document-level guide:
\begin{lstlisting}
var nets = cache(function(numHid, vocabSize, numTopics) {
  var init = nn.constantparams([numHid], 'init');

  var ru = makeRNN(numHid, vocabSize, 'ru');

  var outputHidden = nn.mlp(numHid, [
    {nOut: numHid, activation: nn.tanh}
  ], 'outputHidden');

  var outputMu = nn.mlp(numHid, [
    {nOut: numTopics - 1}
  ], 'outputMu');

  var outputSigma = nn.mlp(numHid, [
    {nOut: numTopics - 1}
  ], 'outputSigma');

  return {
    init: init,
    ru: ru,
    outputHidden: outputHidden,
    outputMu: outputMu,
    outputSigma: outputSigma
  };
});

var model = function(data, vocabSize, numTopics, alpha, eta) {
  var corpus = data.documentsAsCounts;
  var numHid = 20;
  var nets = nets(numHid, vocabSize, numTopics);

  var guideParams = function(topics, doc) {
    var initialState = nneval(nets.init);
    var state = reduce(function(x, prevState) {
      return nneval(nets.ru, [prevState, x]);
    }, initialState, topics.concat(normalize(Vector(doc))));
    var hidden = nneval(nets.outputHidden, state);
    var mu = tensorToVector(nneval(nets.outputMu, hidden));
    var sigma = tensorToVector(softplus(nneval(nets.outputSigma, hidden)));
    var params = {mu: mu, sigma: sigma};
    return params;
  };

  var topics = repeat(numTopics, function() {
    return sample(Dirichlet({alpha: eta}));
  });

  mapData({data: corpus}, function(doc) {

    var topicDist = sample(Dirichlet({alpha: alpha}), {
      guide: LogisticNormal(guideParams(topics, doc))
    });

    mapData({data: countsToIndices(doc)}, function(word) {
      var z = sample(Discrete({ps: topicDist}));
      var topic = topics[z];
      observe(Discrete({ps: topic}), word);
    });

  });

  return topics;
};
\end{lstlisting}


\subsection{Variational Autoencoder}
\label{sec:appendix_code:vae}

In this example and the one that follows, \ic{nnevalModel} evaluates a neural network while also placing an improper uniform prior over the network parameters. This allows neural networks to be used as part of learnable models.

\begin{lstlisting}
// Data
var data = loadData('mnist.json');
var dataDim = 28*28;
var hiddenDim = 500;
var latentDim = 20;

// Encoder
var encodeNet = nn.mlp(dataDim, [
   {nOut: hiddenDim, activation: nn.tanh}
], 'encodeNet');
var muNet = nn.linear(hiddenDim, latentDim, 'muNet');
var sigmaNet = nn.linear(hiddenDim, latentDim, 'sigmaNet');
var encode = function(image) {
   var h = nneval(encodeNet, image);
   return {
      mu: nneval(muNet, h),
      sigma: softplus(nneval(sigmaNet, h))
   };
};

// Decoder
var decodeNet = nn.mlp(latentDim, [
   {nOut: hiddenDim, activation: nn.tanh},
   {nOut: dataDim, activation: nn.sigmoid}
], 'decodeNet');
var decode = function(latent) {
   return nnevalModel(decodeNet, latent);
};

// Training model
var model = function() {
   mapData({data: data, batchSize: 100}, function(image) {
      // Sample latent code (guided by encoder)
      var latent = sample(TensorGaussian({mu: 0, sigma: 1, dims: [latentDim, 1]}), {
         guide: DiagCovGaussian(encode(image))
      });

      // Decode latent code, observe binary image
      var probs = decode(latent);
      observe(MultivariateBernoulli({ps: probs}), image);
   });
}
\end{lstlisting}


\subsection{Sigmoid Belief Network}
\label{sec:appendix_code:sbn}

\begin{lstlisting}
// Data
var data = loadData('mnist.json');
var dataDim = 28*28;
var latentDim = 200;

// Encoder
var encodeNet = nn.mlp(dataDim, [
   {nOut: latentDim, activation: nn.sigmoid}
], 'encodeNet');
var encode = function(image) {
   return nneval(encodeNet, image)
};

// Decoder
var decodeNet = nn.mlp(latentDim, [
   {nOut: dataDim, activation: nn.sigmoid}
], 'decodeNet');
var decode = function(latent) {
   return nnevalModel(decodeNet, latent);
};

// Training model
var priorProbs = Vector(repeat(latentDim, function() { return 0.5; }));
var model = function() {
   mapData({data: data, batchSize: 100}, function(image) {
      // Sample latent code (guided by encoder)
      var latent = sample(MultivariateBernoulli({ps: priorProbs}), {
         guide: MultivariateBernoulli({ps: encode(image)})
      });

      // Decode latent code, observe binary image
      var probs = decode(latent);
      observe(MultivariateBernoulli({ps: probs}), image);
   });
}
\end{lstlisting}


