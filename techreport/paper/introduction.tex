%auto-ignore
\section{Introduction}
\label{sec:introduction}

%% What's our domain?

Probabilistic programs are a powerful tool for probabilistic modeling. A probabilistic programming language (PPL) is a programming language augmented with random sampling and Bayesian conditioning operators. Performing inference on these programs then involves reasoning about the space of executions which satisfy some constraints. A universal PPL is Turing-complete and thus can represent any computable probability distribution, including open-world models, Bayesian non-parameterics, and stochastic recursion~\cite{Church,Venture,Anglican}.

%% What's the problem?

If we consider a probabilistic program to define a distribution $p(\latentVars, \observedVars)$, where $\latentVars$ are (latent) intermediate variable and $\observedVars$ are (observed) output data, then sampling from this distribution is easy: just run the program forward. However, sampling from the posterior distribution $p(\latentVars | \observedVars)$ is hard, involving an intractable integral. Typically, PPLs provide means to approximately sample from the posterior using Monte Carlo methods (e.g. MCMC, SMC).

%% What's the big insight?

These inference methods are so expensive because they (approximately) solve an intractable integral from scratch on every separate invocation.
But many inference problems have shared structure: it is reasonable to expect that computing $p(\latentVars | \observedVars_1)$ should give us some information about how to compute $p(\latentVars | \observedVars_2)$.
In fact, there is reason to believe that this is how people are able to perform certain inferences, such as visual perception, so quickly---we have perceived the world many times before, and can leverage that accumulated knowledge when presented with a new perception task~\cite{AmortizedInference}.
This idea of using the results of previous inferences, or precomputation in general, to make later inferences more efficient is called \emph{amortized inference}~\cite{AmortizedInference,StochasticInverses}.

%% What's our approach?

This paper proposes a system for amortized inference in PPLs. Instead of computing $p(\latentVars | \observedVars)$ from scratch for each $\observedVars$, our system instead constructs a program $\guide(\latentVars | \observedVars)$ which takes $\observedVars$ as input and, when run forward, produces samples distributed approximately according to the true posterior $p(\latentVars | \observedVars)$.
We call $\guide$ a \emph{guide program}, following terminology introduced in previous work~\cite{GuidePrograms}.
The system can spend time up-front constructing a good approximation $\guide$ so that at inference time, sampling from $\guide$ is both fast and accurate.

There is a huge space of possible programs $\guide$ one might consider for this task. Rather than posing the search for $\guide$ as a general program induction problem (as was done in previous work~\cite{GuidePrograms}), we restrict $\guide$ to have the same control flow as the original program $p$, but a different data flow.
That is, $\guide$ samples the same random choices as $p$ and in the same order, but the data flowing into those choices comes from a different computation.
In our system, we represent this computation using neural networks.
This design choice reduces the search for $\guide$ to the much simpler continuous problem of optimizing the weights for these networks, which can be done using stochastic gradient descent.
% The gradients are often high-variance, esp. for programs with discrete choices. To combat this, our system implements several general-purpose variance-reduction strategies.

Our system's interface for specifying guide programs is flexible enough to subsume several popular recent approaches to variational inference, including those that perform both inference and model learning. The system also provides experimental options for automatically deriving guide programs, using recurrent neural networks to capture posterior dependencies.~\remark{Broad statement about the experiments we do and what the results are.}

Our system is implemented in the WebPPL probabilistic programming language~\cite{WebPPL}. Its source code can be found in the WebPPL repository, with additional extensions at \url{https://github.com/probmods/webppl-daipp}.