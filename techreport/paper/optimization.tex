%auto-ignore
\section{Optimizing Parameters}
\label{sec:optimization}

Now that we have seen how to author learnable guide programs, in this section, we will describe how to optimize the parameters of those programs. 

\subsection{ELBo: The Variational Objective}

In Section~\ref{sec:background:variational}, we mentioned that the goal of variational inference is to find values of the parameters $\phi$ for our guide program $\guidePosterior$ such that it is as close as possible to the true posterior $\truePosterior$, where closeness is measured via KL-divergence. The KL-divergence between two general distributions is intractable to compute; however, some straightforward algebra produces an objective that is tractable (following the derivation of Wingate and Weber~\cite{AVIPP}):
%%%
\begin{align}
\begin{split}
\KLD(\guidePosterior || \truePosterior)
&= \int_{\latentVars} \guidePosterior \log \frac{\guide(\guidePosterior}{\truePosterior}\\
&= \int_{\latentVars} \guidePosterior \log \frac{\guidePosterior}{\trueJoint} + \log \dataMarginal\\
&= \expect_{\guide}[ \log(\guidePosterior -  \trueJoint ) ] + \log \dataMarginal\\
&= \log \dataMarginal - \elboDef\\
&= \log \dataMarginal - \elboPhi \geq 0
\end{split}
\label{eq:elbo}
\end{align}
%%%
where the last inequality follows because KL-divergence is non-negative. This in turn implies that $\elboPhi$ is a lower bound on the log marginal likelihood of the data (i.e. evidence) $\log \dataMarginal$. Accordingly, $\elboPhi$ is sometimes referred to as the `Evidence Lower Bound', or ELBo~\cite{BBVI}. Maximizing the ELBo corresponds to minimizing the KL-divergence.

When learning a generative model $\trueJointTheta$, the objective is usually to maximize the log marginal likelihood of the training data $\log \dataMarginalTheta$. Rewriting the result from Equation~\ref{eq:elbo}:
%%%
\begin{equation*}
\elboPhiTheta = \log \dataMarginalTheta - \KLD(\guidePosterior || \truePosteriorTheta)
\end{equation*}
%%%
we can see that, again, because KL-divergence is non-negative, maximizing the ELBo also corresponds to maximizing the log marginal likelihood of the data. Thus, whether we are doing amortized infernence only or amortized inference plus model learning, we can use the same optimization objective: the ELBo.

For an alternative derivation of the ELBo using Jensen's inequality, see Mnih and Gregor~\cite{NVIL} and Jordan et al.~\cite[p. 213]{VariationalInference}.

\subsection{ELBo Gradient Estimators}

Maximizing the ELBo requires estimating its gradient with respect to the parameters. For model learning, computing the gradient with respect to the model parameters $\theta$ is straightforward:
%%%
\begin{align}
\begin{split}
\gradparamsTheta \elboPhiTheta
&= \gradparamsTheta \elboDefGenTheta\\
&= \expect_\guide [ \gradparamsTheta \log \trueJointTheta ] 
\end{split}
\label{eq:modelParamGrad}
\end{align}
%%%
where the expectation with respect to $\guide$ is approximated by samples drawn from $\guide$. This requires that $\trueJointTheta$ be differentiable with respect to $\theta$; many generative models with differentiable likelihoods have this property.

Estimating the gradient with respect to the guide parameters $\phi$ is more complex.
There are two well-known approaches to performing this estimation:

\paragraph{Likelihood Ratio (LR) Estimator:}
In the general case, the gradient of the ELBo with respect to $\phi$ can be estimated by:
%%%
\begin{align}
\begin{split}
\gradparams \elboPhiTheta
&= \gradparams \elboDefGenTheta\\
&= \expect_\guide[ \gradparams \log \guidePosterior ( \log \trueJointTheta - \log \guidePosterior ) ]
\end{split}
\label{eq:lr}
\end{align}
%%%
This estimator goes by the names `likelihood ratio estimator'~\cite{LikelihoodRatioEstimator} and `score function estimator'~\cite{ScoreFunctionEstimator}, and it is also equivalent to the REINFORCE policy gradient algorithm in the reinforcement learning literature~\cite{REINFORCE}. The derivations of this estimator most relevant to our setting can be found in Wingate and Weber~\cite{AVIPP} and Mnih and Gregor~\cite{NVIL}.
Intuitively, each gradient update step using the LR estimator pushes the parameters $\phi$ in the direction $( \log \trueJoint - \log \guidePosterior )$---that is, the direction that will bring the guide closer to the true posterior. The magnitude of this update is controlled by $\gradparams \log \guidePosterior$.

The LR estimator is straightforward to compute, requiring only that $\log \guidePosterior$ be differentiable with respect to $\phi$ (the mean field and neural guide families presented in Section~\ref{sec:background} satisfy this property). However, it is known to exhibit high variance. This problem is amenable to several variance reduction techniques, some of which we will employ later in this section.

\paragraph{Pathwise (PW) Estimator:}

Equation~\ref{eq:lr} is more complicated (and less well-behaved) than Equation~\ref{eq:modelParamGrad} because the gradient can no longer be pushed inside the expectation: the expectation is with respect to $\guide$, and $\guide$ depends on the parameters $\phi$ with respect to which we are differentiating.
However, in certain cases, it is possible to re-write the ELBo such that the expectation distribution does not depend on $\phi$.
This situation occurs whenever the latent variables $\latentVars$ can be expressed as samples from an un-parameterized distribution, followed by a parameterized deterministic transformation:
%%%
\begin{equation*}
\latentVars = \xformedVarsPhi \hspace{2em} \reparamVars \sim \reparamDist(\cdot)
\end{equation*}
%%%
For example, sampling from a Gaussian distribution $\normdist(\mu, \sigma)$ can be expressed as $\mu + \sigma \cdot \reparamVar$, where $\reparamVar \sim \normdist(0, 1)$. Continuous random variables which are parameterized by a location and a scale parameter naturally support this type of transformation, and other types of continuous variables can often be well-approximated by deterministic transformations of unit normal variables~\cite{ADVI}.

Using this `reparameterization trick'~\cite{AEVB} allows the ELBo gradient to be rewritten as:
%%%
\begin{align}
\begin{split}
\gradparams \elboPhiTheta
&= \gradparams \elboDefGenTheta\\
&= \gradparams \expect_\reparamDist [ \log p(\xformedVarsPhi, \observedVars ; \theta) - \log \guide(\xformedVarsPhi | \observedVars ; \phi) ]\\
&= \expect_\reparamDist [\gradparams ( \log p(\xformedVarsPhi, \observedVars ; \theta) - \log \guide(\xformedVarsPhi | \observedVars ; \phi) ) ]
\end{split}
\label{eq:pw}
\end{align}
%%%
This estimator is called the `pathwise derivative estimator'~\cite{PathwiseEstimator}.
It transforms both the guide and target distributions into distributions over independent random `noise' variables $\reparamVars$, followed by complex, parameterized, deterministic transformations. Given a fixed assignment to the noise variables, derivatives can propagate from the final log probabilities back to the input parameters, leading to much more stable gradient estimates than with the LR estimator.

\subsection{Unified Gradient Estimator for Probabilistic Programs}

A general probabilistic program makes many random choices, some of which are amenable to the reparameterization trick (i.e. continuous choices) and others of which are not (i.e. discrete choices). Thus, to optimize the ELBo for a probabilistic program $p$ and an associated guide program $\guide$, we seek a single, unified gradient estimator that handles both discrete and continuous choices and supports model learning as well as amortized inference.

As a first simplifying step, we assume that the generative model and the guide are parameterized by a common set of parameters $\phi$: this reflects the fact that our system internally makes no `generative vs. guide' distinction between parameters. To simplify notation, we drop $\phi$ from all derivations that follow. This is equivalent to making the parameters globally available, which is also true of our system (i.e. \ic{paramScalar} and its ilk can be called anywhere).

Second, we assume that all random choices made by the guide program are first drawn from a distribution $\reparamDist$ and then transformed by a deterministic function $\reparamXform$. Under this assumption, the distribution defined by a guide program factors as:
%%%
\begin{equation*}
\guidePostNoPhi = \prod_i \guide(\reparamXform_i(\reparamVar_i) | \reparamXform_{<i}(\reparamVars_{<i}), \observedVars) \hspace{2em} \reparamVar_i \sim \reparamDist_i(\cdot)
\end{equation*}
%%%
As mentioned in Section~\ref{sec:pplbasics}, the length of $\reparamVars$ can vary across executions. However, since the guide program $\guide$ by construction samples the same random choices in the same order as the target program $p$, $\guide$ factors the same way as $p$ (Equation~\ref{eq:probProgDef}) for any given execution. 

The distributions $\reparamDist_i$ and functions $\reparamXform_i$ have different meanings depending on whether the variable $\reparamVar_i$ is continuous or discrete:
%%%
\begin{itemize}
\item{Continuous $\reparamVar_i$: $\reparamDist_i$ is a unit normal or uniform distribution, and $\reparamXform_i$ is a parameterized transform. This is a direct application of the reparameterization trick. In this case, each local transformation $\reparamXform_i$ may also depend on the previous noise variables $\reparamVars_{<i}$, as choices occuring later in the program may be compound transformations of earlier choices. Appendix~\ref{sec:appendix_reparam} lists some reparameterizable distributions used by our system.}
\item{Discrete $\reparamVar_i$: $\reparamDist_i$ = $\guide_i$, and $\reparamXform_i$ is the identity function. This allows discrete choices to be represented in the reparameterization trick framework.}
\end{itemize}
%%%
Given these assumptions, we can derive an estimator for the ELBo gradient:
%%%
\begin{align}
\label{eq:hybridEstimator}
\gradparams \elbo
%
&= \gradparams \expect_\reparamDist [ \log p(\xformedVars, \observedVars) - \log \guide(\xformedVars | \observedVars) ]\\
%
&= \expect_\reparamDist [ \gradparams \log \reparamDist(\reparamVars | \observedVars) ( \log p(\xformedVars, \observedVars) - \log \guide(\xformedVars | \observedVars) ) + \gradparams( \log p(\xformedVars, \observedVars) - \log \guide(\xformedVars | \observedVars) )]\nonumber\\
%
&= \expect_\reparamDist [ \underbrace{\gradparams \log \reparamDist(\reparamVars | \observedVars) W(\reparamVars, \observedVars)}_{\text{LR term}} + \underbrace{\gradparams \log p(\xformedVars, \observedVars) - \gradparams \log \guide(\xformedVars | \observedVars)}_{\text{PW term}} ]\nonumber
\end{align}
%%%
See Appendix~\ref{sec:appendix:estDerivation} for the derivation. 

This estimator includes the LR and PW estimators as special cases. If all random choices are reparameterized (i.e. they are all continuous), then $\reparamDist(\reparamVars | \observedVars)$ does not depend on $\phi$, thus $\gradparams \log \reparamDist(\reparamVars | \observedVars)$ is zero and the LR term drops out, leaving only the PW term. If no random choices are reparameterized (i.e. they are all discrete), then the $\gradparams \log \guide(\xformedVars | \observedVars)$ term drops out, using the identity $\expect_f[\nabla \log f(x)] = 0$ (see Appendix~\ref{sec:appendix:zeroexp}). If $p$ has no parameters, then the $\gradparams \log p(\xformedVars, \observedVars)$ term is also zero, leaving only the LR term. If $p$ has parameters, then the resulting estimator is the LR estimator plus the model parameter estimator (Equation~\ref{eq:modelParamGrad}).

While Equation~\ref{eq:hybridEstimator} is a correct estimator for the ELBo gradient, like the LR estimator, the presence of discrete (i.e. non-reparameterized) random choices can lead to high variance. Thus we modify this estimator through three variance reduction techniques:
%%%
\begin{enumerate}
\item{Replace $W(\reparamVars, \observedVars)$ with a separate $w_i(\reparamVars, \observedVars)$ for each factor $\reparamDist(\reparamVar_i | \reparamVars_{<i}, \observedVars)$ of $\reparamDist(\reparamVars | \observedVars)$, as there exist independencies that can be exploited to reveal zero-expectation terms in $W(\reparamVars, \observedVars)$ for each $i$.}
\item{Subtract a constant `baseline' term $b_i$ from each $w_i(\reparamVars, \observedVars$). This does not change the expectation, but it can reduce its variance, if designed carefully.}
\item{Factor $\gradparams \log \guide(\xformedVars | \observedVars)$ and remove factors corresponding to discrete (i.e. non-reparameterized) choices, since, as noted above, they have zero expectation}
\end{enumerate}
%%%
Further details and correctness proofs for these three steps can be found in Appendix~\ref{sec:appendix_proofs}. Applying them leads to the following estimator, which is the estimator actually used by our system:
%%%
\begin{align}
\begin{split}
\gradparams \elbo = \expect_\reparamDist \Bigl[ \hspace{0.5em}
&\sum_{i \in \mathcal{D}} \gradparams \log \reparamDist(\reparamVar_i | \reparamVars_{<i}, \observedVars) ( w_i(\reparamVars, \observedVars) - b_i ) \\
%
- &\sum_{i \in \mathcal{C}} \gradparams \log \guide(\reparamXform_i(\reparamVar_i) | \reparamXform_{<i}(\reparamVars_{<i}), \observedVars) \\
%
+ &\gradparams \log p(\xformedVars, \observedVars) \hspace{0.5em} \Bigr]
\end{split}
\label{eq:finalEstimator}
\end{align}
%%%
where $\mathcal{C}$ and $\mathcal{D}$ are the sets of indices for all continuous and discrete random choices in the program execution, respectively.
The estimator naturally factors into a term for discrete choices, a term for continuous choices, and a term for the generative model parameters.

Equation~\ref{eq:finalEstimator} is similar to the `surrogate loss function' gradient estimator of Schulman et al.~\cite{StochasticComputationGraphs}, as an execution of a probabilistic program corresponds to one of their stochastic computation graphs. Their analysis is concerned with general stochastic objectives, however, while we focus particularly on the ELBo.

\remark{(Noah) Argue that this deals properly with open worlds (ie can ignore rvs / params that weren’t touched on a given sample from q). Only (lazily) create parameter when you first hit it, only update it when RVs that it depends on are hit. Need lemma: gradient for param is zero if none of the RV’s it flows into are hit during a run. Again, with our AD, you don’t have to do any special cases to deal with this, it just falls out naturally.}

\subsection{Implementation}

\remark{(Daniel) I feel like we need to bring this section home by making it concrete, describing (in text and/or pseudocode) how we actually calculate the gradient as the program runs. It's easy to describe a simple version of this process (i.e. AD, accumulating into logr, logq, and logp), but the \emph{real} version (using variance reduction + mapData) is more complex: it requires the `dependency' graph, the mapData node stack, and mapData mini-batch multipliers. So I'm conflicted: I want to give the reader something, but a simple version may be misleadingly simple while the full truth may be too complicated to describe. Thoughts? Maybe this section belongs in the Appendix, if it belongs anywhere?}

\subsection{Optimization Interface}

In Section~\ref{sec:pplbasics}, we showed how WebPPL programs use the \ic{Infer} function to perform non-amortized inference on a \ic{model} function. To optimize parameters for amortized inference, WebPPL provides an \ic{Optimize} function with a similar interface:
%%%
\begin{lstlisting}
var model = function() {
   // Use sample, guide, mapData, etc.
};

var params = Optimize(model, {
   steps: 100,
   optMethod: 'adam'
});
\end{lstlisting}
%%%
The code above performs 100 gradient update steps on \ic{model} using the Adam stochastic optimization method~\cite{Adam}.
The return value \ic{params} of this function is a map from parameter names to optimized parameter values.
