// Latent Dirichlet Allocation
// (vanilla version, does not enumerate out choice of topic per word)

// dritchie: Idk how to write a good manual guide for this, so I'm assuming that we're just going
//    to do either mean field or full auto DAIPP with this one...

assert(opts.doModelLearning);

var nHidden = opts.nHidden || 3;
var enumOut = opts.enumOut || false;	// Enumerate out latent topic per word?

var cocolabAbstracts = opts.trainingData || readJSON('techreport/data/cocolabAbstractCorpus.json')
var trainingData = cocolabAbstracts.documentsAsCounts;
var vocabSize = cocolabAbstracts.numWords;
var numTopics = 4;


// Parameter for prior on word distributions per topic.
var eta = Vector(repeat(vocabSize, constF(0.1)));
// Parameter for prior on topic distributions per document.
var alpha = Vector(repeat(numTopics, constF(0.1)));


var model = function() {
	// Top-level parameters
	var wordDistsForTopics = repeat(numTopics, function() {
		return Discrete({ps: makeWeightsGenerative(undefined, eta)});
	});

	// Map over documents
	mapData({data: globalStore.data, batchSize: opts.batchSize}, function(doc) {

		var topicDistForDoc = Discrete({ps: makeWeightsGenerative(undefined, alpha)});

		mapIndexed(function(word, count) {

			if (count > 0) {
				if (!enumOut) {
					var topic = sample(topicDistForDoc, {
						guide: Discrete({ps: discreteMeanFieldParams(numTopics)})
					});
					var wordDistForTopic = wordDistsForTopics[topic];
					factor(count * wordDistForTopic.score(word));
				} else  {
					var wordMarginalDist = Enumerate(function() {
						var topic = sample(topicDistForDoc);
						var wordDistForTopic = wordDistsForTopics[topic];
						return sample(wordDistForTopic);
					});
					factor(count * wordMarginalDist.score(word));
				}
			}

		}, doc);

	});

	return wordDistsForTopics;
};

// Model-specific Optimize options
var optimizeOpts = {
	estimator: {ELBO2: {samples: 3, avgBaselines: true}},
};

// Model-specific quantities to compute / return after optimization
var computeCustomReturns = function(params) {
	// Generate an approximate posterior sample of the word distributions per topic,
	//    report the top N most likely words for each topic.
	var N = 10;
	var wordDistsForTopics = sample(SampleGuide(model, {params: params}));
	var topWordsPerTopic = map(function(wordDist) {
		var indicesWithScores = map(function(i) {
			return {i: i, s: wordDist.score(i)};
		}, wordDist.support());
		var topN = sort(indicesWithScores, gt, function(is) { return is.s; }).slice(0, N);
		return map(function(is) {
			return cocolabAbstracts.indexToWordDict[is.i];
		}, topN);
	}, wordDistsForTopics);
	return {
		topWordsPerTopic: topWordsPerTopic
	};
};