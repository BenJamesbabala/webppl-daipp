assert(opts.doModelLearning);

// This is an attempt to implement the sigmoid belief net described
// in:

// Neural Variational Inference and Learning in Belief Networks
// http://arxiv.org/abs/1402.0030

// Possible differences between this and the model/inference algorithm
// in the paper.

// 1. Variance normalization. Do the fancy optimization methods (e.g.
// adam) give us this (or something similar) anyway?

// 2. Data dependent baselines.

// 3. Average baselines shared across random choices in mapData?

// 4. Regularization for everything? Not mentioned, but I wouldn't be
// surprised if this was done.

// This code gets to about -130 (elbo per datum on the training set)
// after 10K steps, probably fewer. This may be close to the result
// reported in the paper for the version without variance
// normalization and data-dependant baselines?

// Usage:

// webppl examples/sbn.wppl --require .

// Requires the version of webppl in the "new-vi-estimator" branch:
// https://github.com/probmods/webppl/tree/new-vi-estimator

var numhid = opts.numhid || 200;
var numviz = 784;
var priorProbs = T.div(ones([numhid, 1]), 2); // [.5, .5, ...]

// NOTE: this is *not* the same as the multi-layer model in the NVIL paper.
// That one actually has multiple layers of MultivariateBernoullis, and the
//    guide samples them in reverse order.
// Ours always just has one MultivariateBernoulli, but the encode/decode
//    neural nets can have multiple layers

var loadData = function(filename) {
  console.time('load data');
  var data = map(Vector, readJSON(filename));
  console.timeEnd('load data');
  return data;
};

// Can specify dataset in opts, if desired (useful e.g. for only loading dataset
//    once when running a bunch of experiments on model variants)
var trainingData = opts.trainingData || loadData('techreport/data/mnist_images_train.json');
var testData = opts.testData || loadData('techreport/data/mnist_images_test.json');

var dataMean = function(data) {
  var dim = data[0].dims[0];
  var init = zeros([dim, 1]);
  var sum = reduce(function(x, acc) { return T.add(x, acc); }, init, data);
  return T.div(sum, data.length);
};
var preprocessData = function(trainingData, testData) {
  globalStore.meanImage = dataMean(trainingData);
};
var subtractMeanImage = function(image) { return T.sub(image, globalStore.meanImage); };

// To avoid having to type out a bunch of undefined's everywhere.
var makeNNParamGenerativeNamed = function(n, m, name) {
  return makeNNParamGenerative(n, m, undefined, undefined, undefined, undefined, name);
};


// Within mapData, should the model observe or generate?
globalStore.generate = false;

var model = function() {

  // Variational guide params
  var We1 = paramMatrix(numhid, numviz, 0, 0.001, 'We1');
  var be1 = paramMatrix(numhid, 1, 'be1');
  var We2 = paramMatrix(numhid, numhid, 0, 0.001, 'We2');
  var be2 = paramMatrix(numhid, 1, 'be2');

  // Generative params
  var Wd1 = makeNNParamGenerative(numhid, numhid, undefined, undefined, 0, 0.001, 'Wd1');
  var bd1 = makeNNParamGenerativeNamed(numhid, 1, 'bd1');
  var Wd2 = makeNNParamGenerative(numviz, numhid, undefined, undefined, 0, 0.001, 'Wd2');
  var bd2 = makeNNParamGenerativeNamed(numviz, 1, 'bd2');

  var encode = function(datum) {
    var l1 = T.sigmoid(T.add(T.dot(We1, subtractMeanImage(datum)), be1));
    return T.sigmoid(T.add(T.dot(We2, l1), be2));
  };

  var decode = function(h) {
    var l1 = T.sigmoid(T.add(T.dot(Wd1, h), bd1));
    return T.sigmoid(T.add(T.dot(Wd2, l1), bd2));
  };

  var batchSize = Math.min(opts.batchSize || 100, globalStore.data.length);
  return mapData({data: globalStore.data, batchSize: batchSize}, function(datum) {

    var h = sample(MultivariateBernoulli({ps: priorProbs}), {
      guide: MultivariateBernoulli({ps: encode(datum)})
    });

    var ps = decode(h);

    if (globalStore.generate) {
      return ps;
    } else {
      observe(MultivariateBernoulli({ps: ps}), datum);
      return h;
    }
  });

};

// Model-specific Optimize options
var optimizeOpts = {
  steps: 10000,
  // estimator: {ELBO: {samples: 1, avgBaselines: true}},  // Moved to sandbox options
  optMethod: {adam: {stepSize: .001}}
};


// Model-specific quantities to compute / return after optimization

// Encode some test data, then decode it again to see how good
//    the reconstruction is.
var genEncodeDecodeSamples = function(params, testData) {
  globalStore.generate = true;
  var nTargets = opts.sbn_numEncodeDecodeTargets || 5;
  var targetReconstructions = repeat(nTargets, function() {
    var target = testData[randomInteger(testData.length)];
    globalStore.data = [target];
    var nSamps = opts.sbn_numEncodeDecodeSamplesPerTarget || 10;
    var reconstructions =  repeat(nSamps, function() {
      var imgs = sample(ForwardSample(model, {
        samples: 1,
        guide: true,
        params: params
      }));
      return imgs[0]; // because mapData puts it in a list
    });
    return {
      target: target,
      reconstructions: reconstructions
    };
  });
  globalStore.generate = false;
  return {
    sbnEncodeDecodeSamples: targetReconstructions
  };
};

var computeCustomReturns = function(params, testData) {
  return genEncodeDecodeSamples(params, testData);
};
