// Attend, Infer, Repeat
// http://arxiv.org/abs/1603.08575

// The multi-MNIST model

// The guide distribution generated by (default) daipp differs from
// that in the paper in the following ways:

// - 'attention' window via spatial transformer (see below)
// - LSTM (256 cell units)
// - context updated once per-group of scene vars + separate vae? vs.
//   daipp where the context is updated after every random choice, and
//   also has to do the work of the vae encoder network. so the
//   computation performed by the rnn in air is more uniform.
//   (performing a very similar computation at each time step.)

// Unsure about:
// - what (summary) of each group of z is fed back into the recurrent
//   net
// - how hidden states (of the recurrent net) are mapped to guide
//   parameters

// Also need:

// - baselines. (functions of the image and latent variables produced
//   so far.)

var z_what_dim = 50;
var h_dim = '??'; // length of decoder hidden layer
var y_att_dim = 28 * 28;

var x_dims = [50, 50];
var x_sigma = T.add(zeros([product(x_dims), 1]), .3);

var z_where_prior_cov = '??';

var nnparam = function(a, b) {
  return param([a, b], 0, 0.1);
};

var data = load_multi_mnist(); // scaled to [0,1]

var model = function() {

  // TODO: Put a prior over these and do mean-field with point
  // estimates instead of using `nnparam`.

  // Generative parameters.
  var W0 = nnparam(h_dim, z_what_dim);
  var W1 = nnparam(y_att_dim, h_dim);
  var b0 = nnparam(h_dim, 1);
  var b1 = nnparam(y_att_dim, 1);

  var decode = function(z_what) {
    var h = T.tanh(T.add(T.dot(W0, z_what), b0));
    return T.sigmoid(T.add(T.dot(W1, h), b1));
  };

  mapData(data, function(target) {

    initContext('something');

    // Writing the model this way might not work for the DAIR variant.
    // It feeds partial reconstructions into the inference network, so
    // the generation of the scene description and rendering probably
    // need to be interleaved.
    var scene = prior(target, []);
    var image = render(scene, decode);

    factor(DiagCovGaussian({mu: image, sigma: x_sigma}).score(target));

  }, {batchSize: 64});

  return 'something';

};

var prior = function(target, acc) {

  // air feeds the target image into the recurrent net every time a
  // group of scene variables is sampled.
  updateContext(target);

  // Do we really put so much mass on images with zero digits in the
  // prior?
  if (sample(Bernoulli({p: .5}))) {

    // Guide is a Gaussian with full covariance?
    var z_where = sample(MultivariateGaussian({
      mu: zeros([3, 1]),
      cov: z_where_prior_cov
    }));

    // 'attention' window

    // I'm a little confused by fig. 3 (right). It shows x_att
    // depending on the target image only indirectly via the hidden
    // state of the recurrent net, but that doesn't seem to quite fit
    // with the description in the caption. So this is possibly not as
    // close to the original as it could be:
    var x_att = image2att(target, z_where);
    updateContext(x_att);

    var z_what = sample(DiagCovGaussian({
      mu: zeros([z_what_dim, 1]),
      sigma: ones([z_what_dim])
    }));

    return prior(target, acc.concat([z_what, z_where]));

  } else {
    return acc;
  }

};

var render = function(scene, decode) {

  // TODO: Reshape to vector before returning?

  return reduce(function(z, canvas) {

    var z_what = z[0];
    var z_where = z[1];

    var y_att = decode(z_what);

    return T.add(canvas, att2image(y_att, z_where));

  }, zeros(x_dims), scene);

};

// attention window => image
var att2image = function(att, z_where) {
  // shift and scale

  // What exactly does this look like? Described as "spatial
  // transformer" in the paper. (As in "Spatial Transformer
  // Networks"?)

  // TODO: Return matrix to make summing parts easier. (Even though
  // this incurs extra memory allocation?)

};

// image => attention window
var image2att = function(image, z_where) {
};

Optimize(model, {
  method: {adam: {stepSize: 1e-4}} // baselines used 1e-3
});
