/*
This is a simple "explaining away" bayes net. It allows us to test whether the context network learns
to pass along the relevant (summary of) previous choices.

run with: ./webppl examples/bayesnet.wppl --require ./daipp
*/

var targetModel = function(){
  var aStd = 1//sampleDaipp(Gamma({shape: 2, scale: 2}))
  var cStd = 0.5//sampleDaipp(Gamma({shape: 2, scale: 2}))

  var a1 = sample(Gaussian({mu: 0, sigma: aStd}))
  var a2 = sample(Gaussian({mu: 0, sigma: aStd}))

  var c1 = sample(Gaussian({mu: a1+a2, sigma: cStd}))
  // var c2 = sample(Gaussian({mu: b2+b3, sigma: cStd}))
  // var c3 = sample(Gaussian({mu: b1+b3, sigma: cStd}))

  return {c1: c1}
}
var data = repeat(20,targetModel)
display(data)


//NOTE: assume that sampleDaipp(dist,opt) takes a final options arg, and
//when opt.observedVal is not undefined it will be interpretted as a factor
//(except in fantasy mode....).

var model = function(){
  // initContext("modelLearningInitContext")

  var aStd = 1
  var cStd = 0.5

  //map over observations
  var latents = mapData(data,function(datum){
    initContext(datum) //make depend on context in global model?
    var a1 = sampleDaipp(Gaussian({mu: 0, sigma: aStd}))
    var a2 = sampleDaipp(Gaussian({mu: 0, sigma: aStd}))

    var c1 = sampleDaipp(Gaussian({mu: a1+a2, sigma: cStd}), {observedVal: datum.c1})

    return {a1: a1, a2: a2}
  })

  return latents
}



// Tutorial training.
// var marginal = SMC(model, {particles: 100, saveTraces: true, ignoreGuide: true});
// display(marginal.params.dist)
// var params = Optimize(model, {steps: 1000, method: {adagrad: {stepSize: 0.1}}, estimator: {EUBO: {traces: marginal.traces}}});

// VI.
var params = Optimize(model, {
  steps: 10,
  method: {adagrad: {stepSize: 0.01}},
  estimator: {ELBO: {samples: 1}},
  verbose: true});

var posterior = SampleGuide(model, {params: params, samples: 100})

//viz.auto(Enumerate(function(){return sample(posterior)[0]}))

// SampleGuide(model, {samples: 1000, params: params});
//params

var improveParams = function(params) {
  // datumIndex controls which data point will be mapped over by
  // mapData during evaluation.

  // TODO: Compute importance weights for the mini-batch we're about
  // to use for optimization?

  var ess1 = EvaluateGuide(model, {datumIndices: [], params: params, samples: 100});
  // var weights2 = evaluateGuide(model, {datumIndex: 1, params: params});

  display(ess1)
  display('--------------------');

  // Do a few optimization steps.

  return Optimize(model, {
    params: params,
    steps: 100,
    method: {adagrad: {stepSize: 0.01}}, //FIXME: what about state in adagrad, etc?
    estimator: {ELBO: {samples: 1, batchSize: 5}},
    verbose: true
  });
};

var iterate = function(fn, initialVal, n) {
  var iter = function(i, val) {
    return  (i < n) ? iter(i + 1, fn(val)) : val;
  };
  return iter(0, initialVal);
};

// iterate(improveParams, {}, 10);

'done';
