/*
This is a simple "explaining away" bayes net. It allows us to test whether the context network learns
to pass along the relevant (summary of) previous choices.

run with: ./webppl examples/bayesnet.wppl --require ./daipp
*/

var targetModel = function(){
  var aStd = 1//sampleDaipp(Gamma({shape: 2, scale: 2}))
  var bStd = 1//sampleDaipp(Gamma({shape: 2, scale: 2}))
  var cStd = 1//sampleDaipp(Gamma({shape: 2, scale: 2}))

  var a1 = sample(Gaussian({mu: 0, sigma: aStd}))
  var a2 = sample(Gaussian({mu: 0, sigma: aStd}))
  var a3 = sample(Gaussian({mu: 0, sigma: aStd}))

  var b1 = sample(Gaussian({mu: a1+a2, sigma: bStd}))
  var b2 = sample(Gaussian({mu: a2+a3, sigma: bStd}))
  var b3 = sample(Gaussian({mu: a1+a3, sigma: bStd}))

  var c1 = sample(Gaussian({mu: b1+b2, sigma: cStd}))
  var c2 = sample(Gaussian({mu: b2+b3, sigma: cStd}))
  var c3 = sample(Gaussian({mu: b1+b3, sigma: cStd}))

  return {c1: c1, c2: c2, c3: c3} //TODO: randomly mask some of the c's with undefined.
}
var data = repeat(20,targetModel)
display(data)


//NOTE: assume that sampleDaipp(dist,opt) takes a final options arg, and
//when opt.observedVal is not undefined it will be interpretted as a factor
//(except in fantasy mode....).

var model = function(){
  initContext("modelLearningInitContext")
  //model prior, for now just variances per layer
  var aStd = 1//sampleDaipp(Gamma({shape: 2, scale: 2}))
  var bStd = 1//sampleDaipp(Gamma({shape: 2, scale: 2}))
  var cStd = 1//sampleDaipp(Gamma({shape: 2, scale: 2}))

  //map over observations
  var latents = mapData(data,function(datum){
    initContext(datum) //make depend on context in global model?
    var a1 = sampleDaipp(Gaussian({mu: 0, sigma: aStd}))
    var a2 = sampleDaipp(Gaussian({mu: 0, sigma: aStd}))
    var a3 = sampleDaipp(Gaussian({mu: 0, sigma: aStd}))

    var b1 = sampleDaipp(Gaussian({mu: a1+a2, sigma: bStd}))
    var b2 = sampleDaipp(Gaussian({mu: a2+a3, sigma: bStd}))
    var b3 = sampleDaipp(Gaussian({mu: a1+a3, sigma: bStd}))

    var c1 = sampleDaipp(Gaussian({mu: b1+b2, sigma: cStd}), {observedVal: datum.c1})
    var c2 = sampleDaipp(Gaussian({mu: b2+b3, sigma: cStd}), {observedVal: datum.c2})
    var c3 = sampleDaipp(Gaussian({mu: b1+b3, sigma: cStd}), {observedVal: datum.c3})

    return {a1: a1, a2: a2, a3: a3}
  })

  return latents
}



// Tutorial training.
// var marginal = SMC(model, {particles: 100, saveTraces: true, ignoreGuide: true});
// display(marginal.hist)
// var params = Optimize(model, {steps: 1000, method: {adagrad: {stepSize: 0.1}}, estimator: {EUBO: {traces: marginal.traces}}});

// VI.
// var params = Optimize(model, {
//   steps: 100,
//   method: {adagrad: {stepSize: 0.01}},
//   estimator: {ELBO: {samples: 1}},
//   verbose: true});

// SampleGuide(model, {samples: 1000, params: params});
//params

var improveParams = function(params) {
  // datumIndex controls which data point will be mapped over by
  // mapData during evaluation.

  // TODO: Compute importance weights for the mini-batch we're about
  // to use for optimization?

  var ess1 = EvaluateGuide(model, {datumIndices: [], params: params, samples: 100});
  // var weights2 = evaluateGuide(model, {datumIndex: 1, params: params});

  display(ess1)
  display('--------------------');

  // Do a few optimization steps.

  return Optimize(model, {
    params: params,
    steps: 100,
    method: {adagrad: {stepSize: 0.01}}, //FIXME: what about state in adagrad, etc?
    estimator: {ELBO: {samples: 1, batchSize: 5}},
    verbose: true
  });
};

var iterate = function(fn, initialVal, n) {
  var iter = function(i, val) {
    return  (i < n) ? iter(i + 1, fn(val)) : val;
  };
  return iter(0, initialVal);
};

iterate(improveParams, {}, 10);

'done';
