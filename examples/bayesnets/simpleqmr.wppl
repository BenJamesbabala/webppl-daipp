/*
This is a simple "explaining away" bayes net. It allows us to test whether the context network learns
to pass along the relevant (summary of) previous choices.

run with: ./webppl examples/simpleqmr.wppl --require ./daipp
*/

var data = [true, true]

var strengthParam = 0.8
var backgroundParam = 0.05
var noisyOrProb = function(causes,causalPowers,baserate){
  return 1-(1-baserate)*product(map2(function(c,cp){return c?(1-cp):1},causes,causalPowers))
}

var observe = function(dist,val){
  factor(dist.score(val))
}

var model = function() {

  initContext(data);   //initialize the context, incorporating the observation(s)

  var c1=sampleDaipp(Bernoulli({p: 0.1}))
  var c2=sampleDaipp(Bernoulli({p: 0.1}))
  var c3=sampleDaipp(Bernoulli({p: 0.1}))

  observe(Bernoulli({p: noisyOrProb([c1,c3],[strengthParam,strengthParam],backgroundParam)}), data[0])
  observe(Bernoulli({p: noisyOrProb([c2,c3],[strengthParam,strengthParam],backgroundParam)}) ,data[1])

  return [c1,c2,c3];

};


// Tutorial training.
var marginal = SMC(model, {particles: 100, saveTraces: true, ignoreGuide: true});
display(marginal.params.dist)
var params = Optimize(model, {steps: 1000, method: {adagrad: {stepSize: 0.1}}, estimator: {EUBO: {traces: marginal.traces}}});

// VI.
// var params = Optimize(model, {steps: 1000, method: {adagrad: {stepSize: 0.1}}, estimator: 'ELBO'});

SampleGuide(model, {samples: 1000, params: params});
//params

// Enumerate(model)
// Exact solution:
// Marginal:
//     [false,false,true] : 0.6450786794468975
//     [false,true,true] : 0.08512560900245762
//     [true,false,true] : 0.08512560900245762
//     [true,true,false] : 0.07167540882743312
//     [false,true,false] : 0.039819671570796154
//     [true,false,false] : 0.039819671570796154
//     [false,false,false] : 0.02212203976155343
//     [true,true,true] : 0.011233310817608262
