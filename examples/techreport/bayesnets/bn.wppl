// run with: webppl examples/techreport/bayesnets/bn1.wppl --require .


// Options
var nHidden = 3;					// Size of hidden layer for guide recognition net
var nData = 100;					// Number of data points to draw from true distribution
var doModelLearning = false;		// Should we also try to learn the generative model?

// Different model classes
var models = {

	// One continuous latent variable feeding into one continuous observed variable.
	oneLatent: {
		targetModel: function() {
			var a = sample(Gaussian({mu: 0, sigma: 1}));
			var b = sample(Gaussian({mu: a + 2, sigma: 0.5}));
			return {a: a, b: b};
		},
		model: function() {
			// Generative model params
			// TODO: Are Deltas fine, or should we do the mean-field-equivalent thing instead?
			var mu = !doModelLearning ? 0 :
				sample(Gaussian({mu: 0, sigma: 1}), {
					guide: Delta({v: paramScalar()})
				});

			var sigma = !doModelLearning ? 1 :
				sample(Gamma({shape: 1, scale: 1}), {
					guide: Delta({v: toscalar(softplus(paramVector(1)))})
				});

			// Guide params
			var W_hidden = paramMatrix(nHidden, 1);
			var b_hidden = paramVector(nHidden);
			var W_mu = paramMatrix(1, nHidden);
			var b_mu = paramVector(1);
			var W_sigma = paramMatrix(1, nHidden);
			var b_sigma = paramVector(1);

			// Map over data
			var latents = mapData({data: globalStore.data}, function(datum) {
				var observed_b = datum.b;

				// Compute guide params as nonlinear function of datum
				var datumvec = Vector([observed_b]);
				var hidden = sigmoid(linear(datumvec, W_hidden, b_hidden));
				var gmu = toscalar(linear(hidden, W_mu, b_mu));
				var gsigma = toscalar(softplus(linear(hidden, W_sigma, b_sigma)));

				// Sample with guide
				var a = sample(Gaussian({mu: 0, sigma: 1}), {
					guide: Gaussian({mu: gmu, sigma: gsigma})
				});

				// Observe datum
				observe(Gaussian({mu: a + 2, sigma: 0.5}), observed_b);

				// Return latents
				return {a: a};
			});

			// Return global params + latents
			return {
				params: { mu: mu, sigma: sigma },
				latents: latents
			};
		}
	},



};

var whichModel = 'oneLatent';
var model = models[whichModel];


// Generate data from true distribution
globalStore.data = repeat(100, model.targetModel);

// Optimize parameters
var params = Optimize(model.model, {
	steps: 100,
	optMethod: { adam: { stepSize: 0.1 } },
	estimator: { ELBO: { samples: 1 } },
	verbose: true
});

// Compute expected value of params
var paramExpectations = function() {
	globalStore.data = [];
	var post = SampleGuide(model.model, {params: params, samples: 100});
	var paramExample = sample(post).params;
	return mapObject(function(name, val) {
		return expectation(post, function(ret) {
			return ret.params[name];
		});
	}, paramExample);
};

// Compute expected value of latents for given observations
var latentExpectations = function(observations) {
	globalStore.data = [observations];
	var post = SampleGuide(model.model, {params: params, samples: 100});
	var latentExample = sample(post).latents[0];
	return mapObject(function(name, val) {
		return expectation(post, function(ret) {
			return ret.latents[0][name];
		});
	}, latentExample)
};



// Compare param expectations and latent expectations to true values
// TODO: Also compare to expectations computed via MCMC?
display('');
if (doModelLearning) {
	var prmEx = paramExpectations();
	display('Inferred expectation of params: ' + JSON.stringify(prmEx));
}
var testData = repeat(10, model.targetModel);
map(function(testDatum) {
	var latEx = latentExpectations(testDatum);
	display('True datum: ' + JSON.stringify(testDatum) +
		' | Inferred expectation of latents: ' + JSON.stringify(latEx));
}, testData);
undefined;

