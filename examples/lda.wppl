var forEach = function(xs, f) {
  map(function(i) {
    return f(xs[i], i);
  }, _.range(xs.length));
  return;
};

var softplus = function(x) {
  return T.log(T.add(T.exp(x), 1));
};

var normalize = function(x) {
  return T.div(x, T.sumreduce(x));
};

var lda = function(corpus, vocabSize, numTopics, alpha, eta) {

  // In structure.wppl the suggestion is to init with a zero vector.
  // Is there something special about that, or will most anything do?
  initContext([]);

  var topics = repeat(numTopics, function() {
    return sampleDaipp(Dirichlet({alpha: eta}));
    // return sample(Dirichlet({alpha: eta}), {
    //   guide: LogisticNormal({
    //     mu: param([3, 1], 0),
    //     sigma: softplus(param([3, 1], 0))
    //   })
    // });
  });

  var globalContext = globalStore.context;

  mapData(corpus, function(doc) {

    globalStore.context = globalContext;
    updateContext(normalize(Vector(doc)));

    var topicDist = sampleDaipp(Dirichlet({alpha: alpha}));
    // var topicDist = sample(Dirichlet({alpha: alpha}), {
    //   guide: LogisticNormal({
    //     mu: param([3, 1], 0),
    //     sigma: softplus(param([3, 1], 0))
    //   })
    // });


    // forEach, since we don't have nested mapData yet.
    forEach(doc, function(count, word) {

      if (count > 0) {
        var marginal = Enumerate(function() {
          var z = sample(Discrete({ps: topicDist}));
          var topic = topics[z];
          return sample(Discrete({ps: topic}));
        });

        // More efficient summing out of z by moving the factor inside
        // Enumerate.

        // e.g. On bars2 this reduces run time for 200 steps from
        // ~8.8s to ~6.7s.

        // This needs a version of Enumerate which doesn't normalize
        // its result. (Change `normalize` in ScoreAggregator.ad.js and
        // remove the check in `Marginal` in dists.ad.js.)

        // var marginal = Enumerate(function() {
        //   var z = sample(Discrete({ps: topicDist}));
        //   var topic = topics[z];
        //   factor(Discrete({ps: topic}).score(word));
        //   return word;
        // });

        factor(count * marginal.score(word));
      }

    });

  });

  return topics;

};


// Each document is represented by an array of word counts. Therefore
// doc.length == vocabSize, and sum(doc) = no. of words in doc.

var bars = readJSON('daipp/examples/data/bars2.json');

var vocabSize = 4; // V
var numTopics = 4; // K

// Parameter for prior on topic proportions.
var alpha = Vector(repeat(numTopics, constF(0.1)));
// Parameter for prior on topics.
var eta = Vector(repeat(vocabSize, constF(0.1)));

var model = function() {
  return lda(bars, vocabSize, numTopics, alpha, eta);
};

var params = Optimize(model, {
  steps: 1000,
  method: {adagrad: {stepSize: 0.1}},
  estimator: 'ELBO',
  onFinish: function(data) {
    //writeJSON('', data.history)
  }
});

//SampleGuide(model, {samples: 1000, params: params});
// TODO: evaluate how good the result is

'done';
