// Run with:
// webppl --require . --require webppl-fs examples/ldaWithGuide.wppl

var forEach = function(xs, f) {
  map(function(i) {
    return f(xs[i], i);
  }, _.range(xs.length));
  return;
};

var softplus = function(x) {
  return T.log(T.add(T.exp(x), 1));
};

var oneOfK = cache(function(i, k) {
  assert.ok(i >= 0 && i < k);
  var t = zeros([k, 1]);
  t.data.set([1], i);
  return t;
});

var tensorToVector = function(t) {
  // Turn a rank 1 tensor into a WebPPL vector representation.
  var _t = ad.value(t);
  assert.ok(_t.dims.length === 1);
  return T.reshape(t, [_t.dims[0], 1]);
};

var mlp = function(x, params) {
  var h = T.tanh(T.add(T.dot(params.W1, x), params.b1));
  return T.add(T.dot(params.W2, h), params.b2);
};

// This was useful when passing `Vector(doc)` into the context net.
var normalize = function(x) {
  return T.div(x, T.sumreduce(x));
};

var countsToIndices = function(counts) {
  var out = _.flatten(mapIndexed(function(i, count) { repeat(count, constF(i)); }, counts));
  return out;
};

var show = function(x) {
  display(x);
  return x;
};

// *****************************************************************************

// Straight-forward implementation.

var lda0 = function(data, vocabSize, numTopics, alpha, eta) {
  var corpus = data.documents;

  var topics = repeat(numTopics, function() {
    return sample(Dirichlet({alpha: eta}));
  });

  mapData({data: corpus}, function(doc) {

    var topicDist = sample(Dirichlet({alpha: alpha}));

    mapData({data: doc}, function(word) {
      var z = sample(Discrete({ps: topicDist}));
      var topic = topics[z];
      observe(Discrete({ps: topic}), word);
    });

  });

  return topics;
};

// *****************************************************************************

// Straight-forward implementation.
// softplus squishing for parameters of q(z).

var lda0sp = function(data, vocabSize, numTopics, alpha, eta) {
  var corpus = data.documents;

  var topics = repeat(numTopics, function() {
    return sample(Dirichlet({alpha: eta}));
  });

  mapData({data: corpus}, function(doc) {

    var topicDist = sample(Dirichlet({alpha: alpha}));

    mapData({data: doc}, function(word) {
      var z = sample(Discrete({ps: topicDist}), {
        guide: Discrete({ps: softplus(paramVector(numTopics, 0, 0))})
      });
      var topic = topics[z];
      observe(Discrete({ps: topic}), word);
    });

  });

  return topics;
};

// *****************************************************************************

// Straight-forward implementation.
// Not over-parameterized.

var lda0s = function(data, vocabSize, numTopics, alpha, eta) {
  var corpus = data.documents;

  var topics = repeat(numTopics, function() {
    return sample(Dirichlet({alpha: eta}));
  });

  mapData({data: corpus}, function(doc) {

    var topicDist = sample(Dirichlet({alpha: alpha}));

    mapData({data: doc}, function(word) {
      var z = sample(Discrete({ps: topicDist}), {
        guide: Discrete({ps: dists.squishToProbSimplex(paramVector(numTopics - 1, 0, 0))})
      });
      var topic = topics[z];
      observe(Discrete({ps: topic}), word);
    });

  });

  return topics;
};

// *****************************************************************************

// Rather than doing mean-field for z (which creates numDocs *
// numWords parameters) have a guide net that attempts to map directly
// from word => parameters of z.

var lda1 = function(data, vocabSize, numTopics, alpha, eta) {
  var corpus = data.documents;

  // Parameters for the guide net.
  var numHid = 50;
  var netParams = {
    W1: paramMatrix(numHid, vocabSize, 0, 0.1),
    W2: paramMatrix(numTopics, numHid, 0, 0.1),
    b1: paramVector(numHid, 0, 0),
    b2: paramVector(numTopics, 0, 0)
  };

  var wordToParams = function(word) {
    // Use one-of-k encoding for the word. Note that the parameter
    // vector of Discrete is not required to be normalized.
    return {ps: softplus(mlp(oneOfK(word, vocabSize), netParams))};
  };

  var topics = repeat(numTopics, function() {
    return sample(Dirichlet({alpha: eta}));
  });

  mapData({data: corpus}, function(doc) {

    var topicDist = sample(Dirichlet({alpha: alpha}));

    mapData({data: doc}, function(word) {
      var z = sample(Discrete({ps: topicDist}), {
        guide: Discrete(wordToParams(word))
      });
      var topic = topics[z];
      observe(Discrete({ps: topic}), word);
    });

  });

  return topics;
};

// *****************************************************************************

// Same as previous model, but have the guide net also take topicDist
// as input. i.e. attempt to map from from (word, topicDist) =>
// parameters of z.

var lda2 = function(data, vocabSize, numTopics, alpha, eta) {
  var corpus = data.documents;

  // Parameters for the guide net.
  var numHid = 50;
  var netParams = {
    W1: paramMatrix(numHid, vocabSize + numTopics, 0, 0.1),
    W2: paramMatrix(numTopics, numHid, 0, 0.1),
    b1: paramVector(numHid, 0, 0),
    b2: paramVector(numTopics, 0, 0)
  };

  var wordAndTopicDistToParams = function(word, topicDist) {
    // Use one-of-k encoding for the word. Note that the parameter
    // vector of Discrete is not required to be normalized.
    var x = tensorToVector(T.concat(oneOfK(word, vocabSize), topicDist));
    return {ps: softplus(mlp(x, netParams))};
  };

  var topics = repeat(numTopics, function() {
    return sample(Dirichlet({alpha: eta}));
  });

  mapData({data: corpus}, function(doc) {

    var topicDist = sample(Dirichlet({alpha: alpha}));

    mapData({data: doc}, function(word) {
      var z = sample(Discrete({ps: topicDist}), {
        guide: Discrete(wordAndTopicDistToParams(word, topicDist))
      });
      var topic = topics[z];
      observe(Discrete({ps: topic}), word);
    });

  });

  return topics;
};

// *****************************************************************************

// Not over-parameterized.

var lda2s = function(data, vocabSize, numTopics, alpha, eta) {
  var corpus = data.documents;

  // Parameters for the guide net.
  var numHid = 50;
  var netParams = {
    W1: paramMatrix(numHid, vocabSize + numTopics, 0, 0.1),
    W2: paramMatrix(numTopics - 1, numHid, 0, 0.1),
    b1: paramVector(numHid, 0, 0),
    b2: paramVector(numTopics - 1, 0, 0)
  };

  var wordAndTopicDistToParams = function(word, topicDist) {
    // Use one-of-k encoding for the word. Note that the parameter
    // vector of Discrete is not required to be normalized.
    var x = tensorToVector(T.concat(oneOfK(word, vocabSize), topicDist));
    return {ps: dists.squishToProbSimplex(mlp(x, netParams))};
  };

  var topics = repeat(numTopics, function() {
    return sample(Dirichlet({alpha: eta}));
  });

  mapData({data: corpus}, function(doc) {

    var topicDist = sample(Dirichlet({alpha: alpha}));

    mapData({data: doc}, function(word) {
      var z = sample(Discrete({ps: topicDist}), {
        guide: Discrete(wordAndTopicDistToParams(word, topicDist))
      });
      var topic = topics[z];
      observe(Discrete({ps: topic}), word);
    });

  });

  return topics;
};


// *****************************************************************************

// Learn an RNN that chomps along the document generating the
// parameters of all the zs. Initialize the net with the topic dist.

var lda3 = function(data, vocabSize, numTopics, alpha, eta) {
  var corpus = data.documents;

  // Parameters for the guide net.
  var numHid = 10;
  var embedSize = 10;
  var ru = daipp.makeRU('rnn', numHid, embedSize, 'nameofrnn', true);

  var params = {
    init: {
      W: paramMatrix(numHid, numTopics, 0, 0.1),
      b: paramVector(numHid, 0, 0)
    },
    embed: {
      W: paramMatrix(embedSize, vocabSize, 0, 0.1),
      b: paramVector(embedSize, 0, 0)
    },
    predict: {
      W: paramMatrix(numTopics, numHid, 0, 0.1),
      b: paramVector(numTopics, 0, 0)
    }
  };

  // TODO: Clean-up. These are all just a single nn layer.
  var init = function(topicDist) {
    return T.tanh(T.add(T.dot(params.init.W, topicDist), params.init.b));
  };

  var embed = function(word) {
    return T.tanh(T.add(T.dot(params.embed.W, oneOfK(word, vocabSize)), params.embed.b));
  };

  var predict = function(state) {
    return softplus(T.add(T.dot(params.predict.W, tensorToVector(state)), params.predict.b));
  };

  // Model *********************************************************************

  var topics = repeat(numTopics, function() {
    return sample(Dirichlet({alpha: eta}));
  });

  mapData({data: corpus}, function(doc) {

    var topicDist = sample(Dirichlet({alpha: alpha}));

    reduce(function(word, prevState) {
      // Merge word into state.
      var state = daipp.nneval(ru, [prevState, embed(word)]);
      // TODO: Can we still sum out here rather than sampling?
      // TODO: If not summing out, feed sampled value back into net a
      // la daipp?
      var z = sample(Discrete({ps: topicDist}), {
        guide: Discrete({ps: predict(state)})
      });
      var topic = topics[z];
      observe(Discrete({ps: topic}), word);
      return state;
    }, init(topicDist), doc);

  });

  return topics;
};

// *****************************************************************************

// Have a guide net map from topics + counts => params for topicDist.
// Requires docs as counts.

// This (or lda6) might be a good example in which to try taking an
// inference net optimized on one dataset, and using it to initialize
// inference in another? lda6 has the advantage that it would be
// straight-forward to change the number of topics when doing this.
// (Whether it would do anything sensible...)

var lda4 = function(data, vocabSize, numTopics, alpha, eta) {
  var corpus = data.documentsAsCounts;

  var numHid = 100;
  var net1 = nn.mlp(vocabSize * (numTopics + 1), [
    {nOut: numHid, activation: nn.tanh},
  ], 'net1');
  net1.setTraining(true);

  var netMu = nn.mlp(numHid, [
    {nOut: numTopics - 1},
  ], 'netMu');
  netMu.setTraining(true);

  var netSigma = nn.mlp(numHid, [
    {nOut: numTopics - 1},
  ], 'netSigma');
  netSigma.setTraining(true);

  var guideParams = function(topics, doc) {
    //console.log(doc);
    var h = daipp.nneval(net1, T.concat(topics.concat(normalize(Vector(doc)))));
    var mu = tensorToVector(daipp.nneval(netMu, h));
    var sigma = tensorToVector(softplus(daipp.nneval(netSigma, h)));
    var params = {mu: mu, sigma: sigma};
    //console.log(ad.value(mu));
    //console.log(ad.value(sigma));
    return params;
  };

  // Model *********************************************************************

  var topics = repeat(numTopics, function() {
    return sample(Dirichlet({alpha: eta}));
  });

  mapData({data: corpus}, function(doc) {

    var topicDist = sample(Dirichlet({alpha: alpha}), {
      guide: LogisticNormal(guideParams(topics, doc))
    });

    mapData({data: countsToIndices(doc)}, function(word) {
      var z = sample(Discrete({ps: topicDist}));
      var topic = topics[z];
      observe(Discrete({ps: topic}), word);
    });

  });

  return topics;
};

// *****************************************************************************

// The intention here is that this is similar to lda2, but differs by
// having a bigger guide net.

// Instead of combining the topicDist with the one-of-k representation
// of the word, we combine it with a learned word embedding.

var lda5 = function(data, vocabSize, numTopics, alpha, eta) {
  var corpus = data.documents;

  var numHid = 50;
  var embedSize = 50;

  var embedNet = nn.mlp(vocabSize, [{nOut: embedSize, activation: nn.tanh}], 'embedNet');
  embedNet.setTraining(true);

  var net = nn.mlp(embedSize + numTopics, [
    {nOut: numHid, activation: nn.tanh},
    {nOut: numTopics}
  ], 'net');
  net.setTraining(true);

  var wordAndTopicDistToParams = function(word, topicDist) {
    var embedding = daipp.nneval(embedNet, oneOfK(word, vocabSize));
    var out = daipp.nneval(net, T.concat(embedding, T.sub(topicDist, 1)));
    return {ps: softplus(tensorToVector(out))};
  };

  var topics = repeat(numTopics, function() {
    return sample(Dirichlet({alpha: eta}));
  });

  mapData({data: corpus}, function(doc) {

    var topicDist = sample(Dirichlet({alpha: alpha}));

    mapData({data: doc}, function(word) {
      var z = sample(Discrete({ps: topicDist}), {
        guide: Discrete(wordAndTopicDistToParams(word, topicDist))
      });
      var topic = topics[z];
      observe(Discrete({ps: topic}), word);
    });

  });

  return topics;
};

// *****************************************************************************

// A different way (compare lda4) of having a guide net map from
// topics + counts => params for topicDist.

// I choose to have the document be the last input to the RNN so that
// after optimization the RNN could be "partially applied" to the
// topics, resulting in an MLP that can be used to map documents to a
// topic distributions.

var lda6Nets = cache(function(numHid, vocabSize, numTopics) {
  var init = nn.constantparams([numHid]);
  init.setTraining(true);

  var ru = daipp.makeRU('rnn', numHid, vocabSize, 'lda6ru', true);

  var outputHidden = nn.mlp(numHid, [
    {nOut: numHid, activation: nn.tanh}
  ], 'lda6outputHidden');
  outputHidden.setTraining(true);

  var outputMu = nn.mlp(numHid, [
    {nOut: numTopics - 1}
  ], 'lda6outputMu');
  outputMu.setTraining(true);

  var outputSigma = nn.mlp(numHid, [
    {nOut: numTopics - 1}
  ], 'lda6outputSigma');
  outputSigma.setTraining(true);

  return {
    init: init,
    ru: ru,
    outputHidden: outputHidden,
    outputMu: outputMu,
    outputSigma: outputSigma
  };
});

var lda6 = function(data, vocabSize, numTopics, alpha, eta) {
  var corpus = data.documentsAsCounts;
  var numHid = 20;
  var nets = lda6Nets(numHid, vocabSize, numTopics);

  var guideParams = function(topics, doc) {
    var initialState = daipp.nneval(nets.init);
    var state = reduce(function(x, prevState) {
      return daipp.nneval(nets.ru, [prevState, x]);
    }, initialState, topics.concat(normalize(Vector(doc))));
    var hidden = daipp.nneval(nets.outputHidden, state);
    var mu = tensorToVector(daipp.nneval(nets.outputMu, hidden));
    var sigma = tensorToVector(softplus(daipp.nneval(nets.outputSigma, hidden)));
    var params = {mu: mu, sigma: sigma};
    return params;
  };

  // Model *********************************************************************

  var topics = repeat(numTopics, function() {
    return sample(Dirichlet({alpha: eta}));
  });

  mapData({data: corpus}, function(doc) {

    var topicDist = sample(Dirichlet({alpha: alpha}), {
      guide: LogisticNormal(guideParams(topics, doc))
    });

    mapData({data: countsToIndices(doc)}, function(word) {
      var z = sample(Discrete({ps: topicDist}));
      var topic = topics[z];
      observe(Discrete({ps: topic}), word);
    });

  });

  return topics;
};

// *****************************************************************************

// Version that sums out z, i.e. topic assignments.

var lda100 = function(data, vocabSize, numTopics, alpha, eta) {
  var corpus = data.documentsAsCounts;

  var topics = repeat(numTopics, function() {
    return sample(Dirichlet({alpha: eta}));
  });

  mapData({data: corpus}, function(doc) {

    var topicDist = sample(Dirichlet({alpha: alpha}));

    forEach(doc, function(count, word) {

      // Naive summing out.
      // if (count > 0) {
      //   var marginal = Enumerate(function() {
      //     var z = sample(Discrete({ps: topicDist}));
      //     var topic = topics[z];
      //     return sample(Discrete({ps: topic}));
      //   });

      //   factor(count * marginal.score(word));
      // }

      // More efficient summing out of z by not "sampling" w.

      // What I think we might want here is something which like
      // Enumerate that explores all paths (through a thunk), but
      // rather than building a histogram it just returns the (log of)
      // sum of the probability of each path?

      if (count > 0) {
        // Sum over topic assignments/z.
        var prob = sum(mapN(function(z) {
          // In this particular model we could just index into
          // topicDist/topics[z] to get at the log probability.
          var zScore = Discrete({ps: topicDist}).score(z);
          var wgivenzScore = Discrete({ps: topics[z]}).score(word);
          return Math.exp(zScore + wgivenzScore);
        }, numTopics));

        factor(Math.log(prob) * count);
      }

    });

  });

  return topics;

};

var models = {
  lda0: lda0,
  lda0sp: lda0sp,
  lda0s: lda0s,
  lda1: lda1,
  lda2: lda2,
  lda2s: lda2s,
  lda3: lda3,
  lda4: lda4,
  lda5: lda5,
  lda6: lda6,
  lda100: lda100
};

var topWordsInTopics = function(data, model, params) {
  var N = 10;
  var topics = sample(SampleGuide(model, {params: params}));
  var topWordsPerTopic = map(function(topic) {
    // topic is a prob vector here.
    var indicesWithProbs = mapIndexed(function(i, p) {
      return {i: i, p: p};
    }, ad.value(topic).toFlatArray());
    var topN = sort(indicesWithProbs, gt, function(ip) { return ip.p; }).slice(0, N);
    return map(function(ip) {
      return {word: data.indexToWordDict[ip.i], prob: ip.p};
    }, topN);
  }, topics);
  return topWordsPerTopic;
};

var serializeParams = function(params) {
  return JSON.stringify(mapObject(function(k, arr) {
    return map(function(p) {
      return [ad.value(p).toFlatArray(), ad.value(p).dims];
    }, arr);

  }, params));
};

// *****************************************************************************

// TODO: Guide net to predict topic dist from topics and/or doc? (Use
// docs as counts?) Could use this in combination with any of the
// above?

// TODO: Versions of some of the above that feed the topics into the
// guide.

// TODO: Delta's on topics, since folk often don't care about the
// uncertainty here? Requires each eta_i to be >= 1, otherwise the
// "regularization" from log p of the topic dist will force each of
// the topics out to one of the corners of the simplex.

// *****************************************************************************

_.extend(daipp.config, {useXavierInit: true});

var modelName = process.env.MODEL;
assert.ok(_.has(models, modelName), 'Model not found.');
display('Model: ' + modelName);

var path = './tmp/' + Date.now() + '/';
fs.node.mkdirSync(path);
display('Output will be written to ' + path);

// Each document is represented by an array of word counts. Therefore
// doc.length == vocabSize, and sum(doc) = no. of words in doc.

var data = JSON.parse(fs.read('examples/data/cocolabAbstractCorpus.json'));

var vocabSize = data.numWords; // V
var numTopics = process.env.NUM_TOPICS ? _top.parseInt(process.env.NUM_TOPICS) : 5; // K

// Parameter for prior on topic proportions.
var alpha = T.add(zeros([numTopics, 1]), 0.1);
// Parameter for prior on topics.
var eta = T.add(zeros([vocabSize, 1]), 0.1);

var model = function() {
  var f = models[modelName];
  return f(data, vocabSize, numTopics, alpha, eta);
};

var steps = process.env.STEPS ? _top.parseInt(process.env.STEPS) : 10000;
var stepSize = process.env.STEP_SIZE ? _top.parseFloat(process.env.STEP_SIZE) : 0.1;
var optMethod = process.env.OPT_METHOD || 'adam';
var estimator = process.env.ESTIMATOR || 'ELBO2';

var params = {
  steps: steps,
  optMethod: _.object([[optMethod, {stepSize: stepSize}]]),
  // TODO: More than one sample upsets avg. baselines?
  estimator: _.object([[estimator, {samples: 1, avgBaselines: true}]])
};

var allParams = _.defaults({
  modelName: modelName,
  numTopics: numTopics
}, params);

display(JSON.stringify(allParams));

fs.write(path + 'params.txt', JSON.stringify(allParams));

var optParams = Optimize(model, _.defaults({
  onFinish: function(data) {
    fs.write(path + 'history.json', JSON.stringify(data.history));
  }
}, params));

var topWords = topWordsInTopics(data, model, optParams);
fs.write(path + 'top_words.txt', JSON.stringify(topWords));

// Save variational parameters.
fs.write(path + 'parameters.json', serializeParams(optParams));

'done';
